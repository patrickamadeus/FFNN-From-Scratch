{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **NN** Implementation (Forward + Backward Propagation)\n",
    "1. Add input in /test/input and model in /test/model with .txt format, makesure they both have the same filename.\n",
    "2. Input the filename for both the input and the model, ensuring that they have the same filename.\n",
    "3. Scroll down and click on `./tmp/network.html`. This will redirect you to network.html, which you can then open the visualization using a live server.\n",
    "\n",
    "#### Made by:\n",
    "- Samuel Christoper Swandi - 13520075\n",
    "- Grace Claudia - 13520078\n",
    "- Ubaidillah Ariq Prathama - 13520085\n",
    "- Patrick Amadeus Irawan - 13520109\n",
    "\n",
    "------------"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "List of Content\n",
    "\n",
    "1. [Library & Dependencies](#library)\n",
    "2. [Helper Function](#helper)\n",
    "3. [Neural Network Visualization](#visualization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m pip install pyvis==0.3.2\n",
    "!python3 -m pip install networkx==2.6.3\n",
    "!python3 -m pip install numpy==1.21.6"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Library & Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pandas import DataFrame\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "import os\n",
    "\n",
    "from pyvis.network import Network"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Class**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConnectedLayer:\n",
    "    def __init__(self, input_size, output_size, weights):\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.weights = weights[1:]\n",
    "        self.bias = weights[0]\n",
    "\n",
    "    def forward(self, input):\n",
    "        self.input = input\n",
    "        return np.dot(input, self.weights) + self.bias\n",
    "\n",
    "class ActivationLayer:\n",
    "    def __init__(self, activation):\n",
    "        self.activation = activation\n",
    "    \n",
    "    def forward(self, input):\n",
    "        self.input = input\n",
    "        return self.activation(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_output(sample_y_pred, sample_y_train, sample_pred_out, act_type):\n",
    "    if act_type == \"linear\": #linear\n",
    "        dO_dNet = 1\n",
    "    elif act_type == \"ReLU\": #relu\n",
    "        dO_dNet = np.where(sample_y_pred <= 0, 0, 1)\n",
    "    elif act_type == \"sigmoid\": #sigmoid\n",
    "        dO_dNet = sample_y_pred * (1 - sample_y_pred)\n",
    "\n",
    "    dNet_dW = sample_pred_out                   \n",
    "    if act_type != \"softmax\":\n",
    "        dE_dO = -(sample_y_train - sample_y_pred)\n",
    "        dE_dNet = dE_dO * dO_dNet\n",
    "        dE_dW = dE_dNet * dNet_dW\n",
    "    else:\n",
    "        i_max = np.argmax(sample_y_train)\n",
    "\n",
    "        dE_dNet = []\n",
    "        for y_i in range(len(sample_y_train)):\n",
    "            if y_i == i_max:\n",
    "                dE_dNet.append(-(1-sample_y_pred[y_i]))\n",
    "            else:\n",
    "                dE_dNet.append(sample_y_pred[y_i])\n",
    "        \n",
    "        dE_dNet = np.array(dE_dNet)\n",
    "        dE_dW = dE_dNet * dNet_dW\n",
    "\n",
    "    # print(f\"\\n--Calculation {act_type}---\")\n",
    "    # print(\"dE_dO\")\n",
    "    # print(dE_dO)\n",
    "    # print(\"dO_dNet\")\n",
    "    # print(dO_dNet)\n",
    "    # print(\"dE_dNet\")\n",
    "    # print(dE_dNet)\n",
    "    # print(\"dNet_dW\")\n",
    "    # print(dNet_dW)\n",
    "    # print(\"dE_dW\")\n",
    "    # print(dE_dW)\n",
    "\n",
    "    return dE_dW, dE_dNet\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_hidden(succ_dE_dNet, succ_weight, succ_out, sample_pred_out, act_type):\n",
    "    if act_type == \"linear\": #linear\n",
    "        dH_dNet = 1\n",
    "    elif act_type == \"ReLU\": #relu TODO: keknya sala harusnya y, bukan act(y)\n",
    "        dH_dNet = np.where(succ_out <= 0, 0, 1)\n",
    "    elif act_type == \"sigmoid\": #sigmoid\n",
    "        dH_dNet = (succ_out * (1 - succ_out)).flatten()\n",
    "    \n",
    "    # TODO: SOFTMAX\n",
    "\n",
    "    dE_dNet = succ_dE_dNet\n",
    "    dNet_dH = succ_weight\n",
    "    dEtotal_dH = np.sum(dE_dNet * dNet_dH, axis = 1)\n",
    "    dNet_dW = sample_pred_out\n",
    "    dEtotal_dNet = dEtotal_dH * dH_dNet\n",
    "    dEtotal_dW = dEtotal_dNet * dNet_dW\n",
    "\n",
    "    # print(f\"\\n--Calculation {act_type}---\")\n",
    "    # print(\"dE_dNet\")\n",
    "    # print(dE_dNet)\n",
    "    # print(\"dNet_dH\")\n",
    "    # print(dNet_dH)\n",
    "    # print(\"dE_dH\")\n",
    "    # print(dE_dNet * dNet_dH)\n",
    "    # print(\"dEtotal_dH\")\n",
    "    # print(dEtotal_dH)\n",
    "    # print(\"dH_dNet\")\n",
    "    # print(dH_dNet)\n",
    "    # print(\"dNet_dW\")\n",
    "    # print(dNet_dW)\n",
    "    # print(\"dEtotal_dNet\")\n",
    "    # print(dEtotal_dNet)\n",
    "    # print(\"dEtotal_dW\")\n",
    "    # print(dEtotal_dW)\n",
    "\n",
    "    return dEtotal_dW, dEtotal_dNet"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import ceil \n",
    "\n",
    "class NeuralNetwork:\n",
    "    def __init__(self):\n",
    "        self.layers = []\n",
    "\n",
    "    def add(self, layer):\n",
    "        self.layers.append(layer)\n",
    "    \n",
    "    def predict(self, x):\n",
    "        complete_result = []\n",
    "        output = x[:, 1:]\n",
    "\n",
    "        for layer in self.layers:\n",
    "            output = layer.forward(output)\n",
    "            if isinstance(layer, ActivationLayer):\n",
    "                complete_result.append(output)\n",
    "\n",
    "        return complete_result[-1] , complete_result\n",
    "    \n",
    "    def print_weight(self):\n",
    "        i = 0\n",
    "        for layer in self.layers:\n",
    "            if not isinstance(layer, ActivationLayer):\n",
    "                print(\"|____Layer {}___|\".format(i))\n",
    "                print(\"---B I A S---\")\n",
    "                print(layer.bias)\n",
    "                print(\"---N E U R O N S---\")\n",
    "                print(layer.weights)\n",
    "                i+=1\n",
    "            print()\n",
    "\n",
    "    def fit(self, x_train, y_train, learning_rate=0.1, epochs = 5, batch_size = 2, err_threshold = 0.01):\n",
    "        # batch size validation\n",
    "        if batch_size > len(x_train) or batch_size <= 0:\n",
    "            print(\"Batch size {} invalid.\".format(batch_size))\n",
    "            print(\"Batch size range [{}...{}]\".format(1,len(x_train)))\n",
    "            return\n",
    "\n",
    "        print(\"WEIGHT ORI\")\n",
    "        self.print_weight()\n",
    "        print(\"initial error=\", sse(predict_output(x_train, self)[0] / len(x_train), y_train))\n",
    "\n",
    "        # print layer type for ActivationLayer only\n",
    "        act_layers = []\n",
    "        for i in range(len(self.layers)):\n",
    "            if isinstance(self.layers[i], ActivationLayer):\n",
    "                act_layers.append(self.layers[i].activation.__name__)\n",
    "        act_layers.reverse()\n",
    "        print(\"\\nActivation Layers From Back to Front: \", act_layers)\n",
    "\n",
    "        if act_layers[0] != \"softmax\":\n",
    "            init_err = sse(predict_output(x_train, self)[0], y_train)\n",
    "        else:\n",
    "            init_err = cross_entropy(np.max(predict_output(x_train, self)[0]))\n",
    "            \n",
    "        # if initial error already below threshold, stop training\n",
    "        if init_err <= err_threshold:\n",
    "            print(\"Initial error already below threshold, no further training needed.\")\n",
    "            return\n",
    "\n",
    "\n",
    "        print()\n",
    "        # training loop\n",
    "        for i in range(epochs):  # Epoch Loop ##################11111111111#############\n",
    "            # print(\"- - - - - - - - - - - - - - - - EPOCH {} - - - - - - - - - - - - - - - -\".format(i+1))\n",
    "            # err = 0\n",
    "            start_i = 0\n",
    "            end_i = batch_size\n",
    "\n",
    "            for j in range(ceil(len(x_train) / batch_size)):  # Mini Batch Loop ###########22222222222############\n",
    "                # print(\"============BATCH {}==============\".format(j + 1))\n",
    "                batch_x_train = x_train[start_i:end_i]\n",
    "                batch_y_train = y_train[start_i:end_i]\n",
    "\n",
    "\n",
    "                # FORWARD PROPAGATION\n",
    "                batch_y_pred, complete_out = predict_output(batch_x_train, self)\n",
    "                batch_y_pred = np.array(batch_y_pred)\n",
    "                # err += sse(batch_y_pred, batch_y_train)\n",
    "\n",
    "                # print(\"====FORWARD PROPAGATION====\")\n",
    "                # print(\"Batch X Train\")\n",
    "                # print(batch_x_train)\n",
    "                # print(\"Batch Y Train\")\n",
    "                # print(batch_y_train)\n",
    "                # print(\"Batch Y pred\")\n",
    "                # print(batch_y_pred)\n",
    "\n",
    "                # BACKWARD PROPAGATION\n",
    "                out_grad = None\n",
    "                first_hid_grad = True\n",
    "                hid_grad = []\n",
    "\n",
    "                for k in range(len(batch_x_train)):  # Single Sample Backpropagation Loop ########333333333######\n",
    "                    sample_y_pred = batch_y_pred[k]\n",
    "                    sample_y_train = batch_y_train[k]\n",
    "\n",
    "                    #-------------- Output Gradient --------------#\n",
    "                    if len(self.layers) == 2:\n",
    "                        prev_out = batch_x_train[k][1:].reshape(-1,1)\n",
    "                    else:\n",
    "                        prev_out = uniform2D(np.array(complete_out)[k, -2]).reshape(-1,1)\n",
    "                    \n",
    "                    # Duplicate predecessor to match output layer neuron size & append bias\n",
    "                    succ_size = len(complete_out[0][-1])\n",
    "                    sample_pred_out = np.hstack([prev_out] * succ_size)\n",
    "                    sample_pred_out = np.vstack([np.ones(succ_size),sample_pred_out])\n",
    "\n",
    "                    # print(\"\\n===Batch {} Sample {}===\".format(j+1,k+1))\n",
    "\n",
    "                    # print(\"\\n--Output Materials--\")\n",
    "                    # print(\"sample_y_pred\")\n",
    "                    # print(sample_y_pred)\n",
    "                    # print(\"sample_y_train\")\n",
    "                    # print(sample_y_train)\n",
    "                    # print(\"sample_pred_out\")\n",
    "                    # print(sample_pred_out)\n",
    "                    # print(\"Output Layer Type\")\n",
    "                    # print(act_layers[0])\n",
    "\n",
    "                    dE_dW, succ_dE_dNet = grad_output(sample_y_pred, sample_y_train, sample_pred_out, act_layers[0]) # TODO CHANGE NOT SIGMOID\n",
    "                    if out_grad is None:\n",
    "                        out_grad = dE_dW\n",
    "                    else:\n",
    "                        out_grad += dE_dW\n",
    "\n",
    "                    # -------------- Hidden Gradient --------------#\n",
    "                    hid_grad_i = 0\n",
    "                    reversed_i = -2\n",
    "\n",
    "                    # TODO : Testing\n",
    "                    for l_i in range(len(list_layer_size) - 1):   \n",
    "                        if l_i == len(list_layer_size) - 2:\n",
    "                            prev_out = np.array(batch_x_train[k][1:]).reshape(-1,1)  # cut the bias first\n",
    "                        else:\n",
    "                            prev_out = uniform2D(np.array(complete_out)[k, reversed_i]).reshape(-1,1)\n",
    "                        \n",
    "                        succ_out = np.array(complete_out)[k, reversed_i].reshape(-1,1)\n",
    "                        succ_weight = np.array(self.layers[reversed_i].weights)\n",
    "                        # Duplicate predecessor to match succ layer neuron size & append bias\n",
    "                        succ_size = len(complete_out[0][reversed_i])\n",
    "                        sample_pred_out = np.hstack([prev_out] * succ_size)\n",
    "                        sample_pred_out = np.vstack([np.ones(succ_size),sample_pred_out])\n",
    "\n",
    "                        # print(\"\\n--Hidden Materials--\")\n",
    "                        # print(\"sample_pred_out\")\n",
    "                        # print(sample_pred_out)\n",
    "                        # print(\"succ_out\")\n",
    "                        # print(succ_out)\n",
    "                        # print(\"succ_dE_dNet\")\n",
    "                        # print(succ_dE_dNet)\n",
    "                        # print(\"succ_weight\")\n",
    "                        # print(succ_weight)\n",
    "\n",
    "                        dEtotal_dW, succ_dEtotal_dNet = grad_hidden(succ_dE_dNet, succ_weight, succ_out, sample_pred_out, act_layers[l_i + 1])\n",
    "                        if first_hid_grad:\n",
    "                            hid_grad.append(dEtotal_dW)\n",
    "                        else:\n",
    "                            hid_grad[hid_grad_i] += dEtotal_dW\n",
    "                            hid_grad_i += 1\n",
    "\n",
    "                    first_hid_grad = False\n",
    "                \n",
    "                # print(\"Out grad\")\n",
    "                # print(out_grad)\n",
    "                # print(\"Hid grad\")\n",
    "                # print(hid_grad)\n",
    "\n",
    "                out_delta = -learning_rate * out_grad\n",
    "                hid_delta = -learning_rate * np.array(hid_grad)\n",
    "\n",
    "                # print()\n",
    "                # print(\"Out_delta\")\n",
    "                # print(out_delta)\n",
    "                # print(\"Hid_delta\")\n",
    "                # print(hid_delta)\n",
    "\n",
    "                # reverse hidden delta\n",
    "                deltas = (hid_delta[::-1].tolist() + [out_delta.tolist()])\n",
    "                for d_i in range(len(deltas)):\n",
    "                    deltas[d_i] = np.array(deltas[d_i])\n",
    "                # print(\"Deltas\")\n",
    "                # print(deltas)\n",
    "\n",
    "                # Update weight\n",
    "                d_i = 0\n",
    "                for layer in self.layers:\n",
    "                    # if isinstance(layer, ActivationLayer):\n",
    "                    #     print(layer.activation.__name__)\n",
    "                    if not isinstance(layer, ActivationLayer):\n",
    "                        # print(\"\\n>>>>>>>>>BEFORE\")\n",
    "                        # print(\"Layer Bias\")\n",
    "                        # print(layer.bias)\n",
    "                        # print(\"Layer Weights\")\n",
    "                        # print(layer.weights)\n",
    "                        # print(\"\\nDelta\")\n",
    "                        # print(deltas[d_i][:])\n",
    "                        layer.bias += deltas[d_i][0]\n",
    "                        layer.weights += deltas[d_i][1:]\n",
    "                        # print(\"\\n>>>>>>>>>>AFTER\")\n",
    "                        # print(\"Layer Bias\")\n",
    "                        # print(layer.bias)\n",
    "                        # print(\"Layer Weights\")\n",
    "                        # print(layer.weights)\n",
    "                        d_i+=1  \n",
    "\n",
    "                # Increment next batch\n",
    "                start_i += batch_size\n",
    "                end_i += batch_size\n",
    "                # Verbose Weight per mini-batch\n",
    "                # self.print_weight()\n",
    "            \n",
    "            if act_layers[0] == \"softmax\":\n",
    "                err = cross_entropy(np.max(predict_output(x_train, self)[0]))\n",
    "            else:\n",
    "                err = sse(predict_output(x_train, self)[0], y_train)\n",
    "\n",
    "            print(\"Epoch {}/{}   error={}\".format(i+1,epochs, err/ len(x_train)))\n",
    "            if err/ len(x_train) < err_threshold:\n",
    "                print(\"Curr error < threshold. Finishing Epoch\")\n",
    "                break\n",
    "        print(\"\\n----------------------------------------------------------\")\n",
    "        print(\"============|==========FINAL WEIGHT===========|===========\")\n",
    "        print(\"----------------------------------------------------------\")\n",
    "\n",
    "        self.print_weight()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def uniform2D(arr):\n",
    "    return np.array([np.array(i) for i in arr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear(net):\n",
    "    return net\n",
    "\n",
    "def ReLU(net):\n",
    "    return np.maximum(0,net)\n",
    "\n",
    "def sigmoid(net):\n",
    "    return 1/(1+np.exp(-net))\n",
    "\n",
    "def softmax(net):\n",
    "    res = []\n",
    "    for sample in net:\n",
    "        res.append(np.exp(sample)/np.sum(np.exp(sample)))\n",
    "    return np.array(res)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse(y_true, y_pred):\n",
    "    return np.mean(np.square(y_true - y_pred))\n",
    "\n",
    "def sse(y_true, y_pred):\n",
    "    return np.sum(np.square(y_true - y_pred)) / 2\n",
    "\n",
    "def cross_entropy(pk):\n",
    "    return -np.log(pk)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Loading Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_load(data):\n",
    "    '''\n",
    "    Function to load model from file\n",
    "    INPUT :     data -> data from model file\n",
    "\n",
    "    OUTPUT :    weights -> weights of each model neuron\n",
    "                list_prev_size -> list of previous layer size\n",
    "                list_layer_size -> list of current layer size\n",
    "                list_activation -> list of activation function\n",
    "    '''\n",
    "\n",
    "    idx = 1\n",
    "    weights,list_prev_size,list_layer_size,list_activation = [],[],[],[]\n",
    "\n",
    "    for i in range(int(data[0]) - 1):\n",
    "        # Loading size & act function\n",
    "        prev_size, layer_size, activation = [int(i) for i in data[idx].split()]\n",
    "        list_prev_size.append(prev_size)\n",
    "        list_layer_size.append(layer_size)\n",
    "        list_activation.append(activation)\n",
    "        \n",
    "        # Loading weights\n",
    "        idx += 1\n",
    "        weight = []\n",
    "        for j in range(prev_size + 1):\n",
    "            weight.append([float(i) for i in data[idx].split()])\n",
    "            idx += 1\n",
    "        weights.append(weight)\n",
    "    \n",
    "    # Verbose Information\n",
    "    print('---Model Information---')\n",
    "    print('Number of layers :', len(list_prev_size) + 1)\n",
    "    print('Input size :', list_prev_size[0])\n",
    "    print('Output size :', list_layer_size[-1])\n",
    "\n",
    "    print()\n",
    "    print('Weights :', weights)\n",
    "    print('Previous layer size :', list_prev_size)\n",
    "    print('Current layer size :', list_layer_size)\n",
    "    print('Activation function :', list_activation)\n",
    "\n",
    "    return weights, list_prev_size, list_layer_size, list_activation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Input Loading Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_x_train(input_data):\n",
    "    x_train = []\n",
    "    # Parse each line\n",
    "    for i in range(len(input_data)):\n",
    "        x_train.append([float(i) for i in input_data[i].split()])\n",
    "\n",
    "    return np.array(x_train)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predict Output Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_output(x, net):\n",
    "    '''\n",
    "    Function to predict output from input data\n",
    "    INPUT :     x-> input data\n",
    "                net -> NeuralNetwork object\n",
    "\n",
    "    OUTPUT :    out -> output of the model\n",
    "                complete_out -> complete output of the model, visualization purpose\n",
    "    '''\n",
    "    # Predict output\n",
    "    out, complete_out = net.predict(x)\n",
    "\n",
    "    # Gather complete output\n",
    "    n_complete_out = []\n",
    "    for i in range(len(complete_out[0])):\n",
    "        n_complete_out.append([complete_out[0][i], complete_out[-1][i]])\n",
    "\n",
    "    complete_out = n_complete_out\n",
    "    return out, complete_out"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SSE Errors Function + microhelper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_out_and_errors(output_data, out):\n",
    "    '''\n",
    "    Function to calculate output error\n",
    "    INPUT :     output_data -> output data from output file\n",
    "                out -> output of the model, obtained from predict_output function\n",
    "\n",
    "    OUTPUT :    out_pred -> output of the model (flattened)\n",
    "                out_true -> output from output_data (flattened)\n",
    "                sse_error -> sum squared error\n",
    "                sse_error <= max_sse -> boolean value, True if sse_error <= max_sse, False otherwise\n",
    "    '''\n",
    "    # Assign y_train from output_data\n",
    "    parsed_output = [[float(j) for j in i.split()] for i in output_data]\n",
    "    out_pred = out.flatten()\n",
    "    out_true = np.array(parsed_output[:-1]).flatten()\n",
    "    max_sse = parsed_output[-1][0]\n",
    "\n",
    "    return out_pred, out_true, sse(out_true, out_pred), sse(out_true, out_pred) <= max_sse"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Load** Section"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entrypoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_FOLDER = '../test/model/'\n",
    "INPUT_FOLDER = '../test/input/'\n",
    "OUTPUT_FOLDER = '../test/output/'\n",
    "# read file from test folder\n",
    "def read_file(folder_path, file_name):\n",
    "    with open(folder_path + file_name, 'r') as file:\n",
    "        data = [i.rstrip(\"\\n\") for i in file.readlines()]\n",
    "    return data\n",
    "\n",
    "filename = input('Enter test case name (with extension): ')\n",
    "data = read_file(MODEL_FOLDER, filename)\n",
    "input_data = read_file(INPUT_FOLDER, filename)\n",
    "\n",
    "try: \n",
    "    output_data = read_file(OUTPUT_FOLDER, filename)\n",
    "except:\n",
    "    output_data = None"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model_load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights, list_prev_size, list_layer_size, list_activation = model_load(data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build NN model\n",
    "act = {0: linear, 1: ReLU, 2: sigmoid, 3: softmax}\n",
    "net = NeuralNetwork()\n",
    "for i in range (int(data[0]) - 1):\n",
    "    net.add(ConnectedLayer(list_prev_size[i], list_layer_size[i], weights[i]))\n",
    "    net.add(ActivationLayer(act[list_activation[i]]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forward Propagation - TUBES BAGIAN A section"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Predict** Section"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare x_train & predict output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = generate_x_train(input_data)\n",
    "out, complete_out = predict_output(x_train, net)\n",
    "\n",
    "# Print the information, complete with brief verbose\n",
    "print('---Prediction Information---')\n",
    "print('Input data :', x_train)\n",
    "print('Output data :', out)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute Error with **Sum Squared Error (SSE)**\n",
    "\n",
    "Sum Squared Error (SSE) is a mathematical function used in statistics and machine learning to measure the difference between predicted and actual values. It is commonly used as a cost function in various optimization algorithms, such as gradient descent.\n",
    "\n",
    "The SSE is calculated by taking the difference between each predicted value and its corresponding actual value, squaring the difference, and then summing all of the squared differences:\n",
    "\n",
    "$$SSE = \\sum_{i=1}^{n}(y_i - \\hat{y}_i)^2$$\n",
    "\n",
    "Where:\n",
    "- $n$ is the number of data points\n",
    "- $y_i$ is the actual value of the i-th data point\n",
    "- $\\hat{y}_i$ is the predicted value of the i-th data point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_pred, out_true, sse_err, isLessThanMaxSSE = compute_out_and_errors(output_data, out)\n",
    "\n",
    "print('Output prediction :', out_pred)\n",
    "print('Output true :', out_true)\n",
    "print('SSE  : ', sse_err)\n",
    "print(\"sse <= max_sse  :\", isLessThanMaxSSE)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network Visualization with **Pyvis**\n",
    "\n",
    "Pyvis is a Python library that provides an easy-to-use interface for visualizing complex networks, including neural networks. Implementation of the visualization is enlisted below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_network(data, complete_out) -> Network:\n",
    "    n = int(data[0])\n",
    "\n",
    "    # Constant\n",
    "    XSTEP, YSTEP, SIZE = 300, 300, 10\n",
    "\n",
    "    # Nodes\n",
    "    nodes = []\n",
    "    node_i = 1\n",
    "\n",
    "    # Nodes Value\n",
    "    value = []\n",
    "    x_val = 0\n",
    "    y_val = 0\n",
    "\n",
    "    # Position\n",
    "    x = []\n",
    "    y = []\n",
    "\n",
    "    # Styling + Text\n",
    "    label = []\n",
    "    color = []\n",
    "    edge = []\n",
    "    title = []\n",
    "\n",
    "    # Indexing\n",
    "    src_idx = 0\n",
    "    idx = 1\n",
    "\n",
    "    for i_layer in range(n - 1):\n",
    "        n_curr, n_next, _ = [int(i) for i in data[idx].split()]\n",
    "\n",
    "        if i_layer == 0:  # means that this is the first layer, hence construct input\n",
    "            for i_node in range(n_curr + 1):\n",
    "                if i_node == 0:  # bias\n",
    "                    color.append(\"#dd4b39\")\n",
    "                    label.append(\"Input[bias]\")\n",
    "                    temp = \"\"\n",
    "                    for i in range (len(input_data)):\n",
    "                        temp += str(x_train[i][0])\n",
    "                        if i < len(input_data) - 1:\n",
    "                            temp += \", \"\n",
    "                    title.append(temp)\n",
    "                else:\n",
    "                    color.append(\"#162347\")\n",
    "                    label.append(\"Input[{}]\".format(i_node))\n",
    "                    temp = \"\"\n",
    "                    for i in range (len(input_data)):\n",
    "                        temp += str(x_train[i][i_node])\n",
    "                        if i < len(input_data) - 1:\n",
    "                            temp += \", \"\n",
    "                    title.append(temp)\n",
    "                value.append(SIZE)\n",
    "                x.append(x_val)\n",
    "                y.append(y_val)\n",
    "                y_val += YSTEP\n",
    "                nodes.append(node_i)\n",
    "                node_i += 1\n",
    "            x_val += XSTEP\n",
    "\n",
    "        y_val = 0\n",
    "        # always construct the next layer\n",
    "        for i_node in range(n_next + 1):\n",
    "            if i_node == 0:\n",
    "                if i_layer == n - 2:\n",
    "                    continue\n",
    "                color.append(\"#dd4b39\")\n",
    "                label.append(\"HL{}[bias]\".format(i_layer + 1))\n",
    "                temp = \"\"\n",
    "                for i in range (len(input_data)):\n",
    "                    temp +=  \"1\"\n",
    "                    if i < len(input_data) - 1:\n",
    "                        temp += \", \"\n",
    "                title.append(temp)\n",
    "            else:\n",
    "                color.append(\"#162347\")\n",
    "                if i_layer == n - 2:\n",
    "                    label.append(\"Output[{}]\".format(i_node))\n",
    "                else:\n",
    "                    label.append(\"HL{}[{}]\".format(i_layer + 1, i_node))\n",
    "                temp = \"\"\n",
    "                for i in range (len(input_data)):\n",
    "                    temp += str(complete_out[i][i_layer][i_node - 1])\n",
    "                    if i < len(input_data) - 1:\n",
    "                        temp += \", \"\n",
    "                title.append(temp)\n",
    "            value.append(SIZE)\n",
    "            x.append(x_val)\n",
    "            y.append(y_val)\n",
    "            y_val += YSTEP\n",
    "            nodes.append(node_i)\n",
    "            node_i += 1\n",
    "        x_val += XSTEP\n",
    "\n",
    "        idx += 1\n",
    "        for origin in range(n_curr + 1):\n",
    "            dst_idx = -1\n",
    "            for w in reversed(data[idx].split()):\n",
    "                edge.append((nodes[src_idx], nodes[dst_idx], w))\n",
    "                dst_idx -= 1\n",
    "            src_idx += 1\n",
    "            idx += 1\n",
    "\n",
    "    g = Network(notebook=True, cdn_resources=\"remote\")\n",
    "    g.add_nodes(nodes,title = title,value = value,x=x,y=y,label = label,color = color)\n",
    "\n",
    "    for e in edge:\n",
    "        g.add_edge(e[0], e[1], title = e[2], color=\"#162347\")\n",
    "\n",
    "    for n in g.nodes:\n",
    "        n.update({'physics': False})\n",
    "    \n",
    "    return g"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize using Helper Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = visualize_network(data, complete_out)\n",
    "g.show(\"./tmp/network.html\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Backward Propagation** - TUBES BAGIAN B Section"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Repeated** model load + NN construct to speed up code running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Model Information---\n",
      "Number of layers : 2\n",
      "Input size : 2\n",
      "Output size : 2\n",
      "\n",
      "Weights : [[[0.15, 0.25], [0.2, 0.3], [0.35, 0.35]]]\n",
      "Previous layer size : [2]\n",
      "Current layer size : [2]\n",
      "Activation function : [2]\n",
      "\n",
      "---Prediction Information---\n",
      "Input data : [[1.  0.  0. ]\n",
      " [1.  0.  0.1]\n",
      " [1.  1.  0. ]\n",
      " [1.  1.  1. ]]\n",
      "Output prediction : [0.53742985 0.5621765  0.54611854 0.57077161 0.58661758 0.63413559\n",
      " 0.66818777 0.7109495 ]\n",
      "Output true : [0.1 1.  1.  0.  1.  1.  0.  0. ]\n",
      "SSE  :  1.0857443918420457\n",
      "sse <= max_sse  : False\n"
     ]
    }
   ],
   "source": [
    "MODEL_FOLDER = '../test/model/'\n",
    "INPUT_FOLDER = '../test/input/'\n",
    "OUTPUT_FOLDER = '../test/output/'\n",
    "# read file from test folder\n",
    "def read_file(folder_path, file_name):\n",
    "    with open(folder_path + file_name, 'r') as file:\n",
    "        data = [i.rstrip(\"\\n\") for i in file.readlines()]\n",
    "    return data\n",
    "\n",
    "TC_B = \"\"\n",
    "tc_b = input(\"Directly Enter if you wish to access Test Case B folder (otherwise type anything): \")\n",
    "if tc_b == \"\":\n",
    "    MODEL_FOLDER += \"B/\"\n",
    "    INPUT_FOLDER += \"B/\"\n",
    "    OUTPUT_FOLDER += \"B/\"\n",
    "\n",
    "filename = input('Enter test case name (with extension): ')\n",
    "data = read_file(MODEL_FOLDER, filename)\n",
    "input_data = read_file(INPUT_FOLDER, filename)\n",
    "\n",
    "try: \n",
    "    output_data = read_file(OUTPUT_FOLDER, filename)\n",
    "except:\n",
    "    output_data = None\n",
    "\n",
    "# Load model building blocks\n",
    "weights, list_prev_size, list_layer_size, list_activation = model_load(data)\n",
    "\n",
    "# Build NN Model\n",
    "act = {0: linear, 1: ReLU, 2: sigmoid, 3: softmax}\n",
    "net = NeuralNetwork()\n",
    "for i in range (int(data[0]) - 1):\n",
    "    net.add(ConnectedLayer(list_prev_size[i], list_layer_size[i], weights[i]))\n",
    "    net.add(ActivationLayer(act[list_activation[i]]))\n",
    "\n",
    "# Generate x_train\n",
    "x_train = generate_x_train(input_data)\n",
    "out, complete_out = predict_output(x_train, net)\n",
    "\n",
    "# Print the information, complete with brief verbose\n",
    "print('\\n---Prediction Information---')\n",
    "print('Input data :', x_train)\n",
    "\n",
    "# Compute out error\n",
    "out_pred, out_true, sse_err, isLessThanMaxSSE = compute_out_and_errors(output_data, out)\n",
    "\n",
    "print('Output prediction :', out_pred)\n",
    "print('Output true :', out_true)\n",
    "print('SSE  : ', sse_err)\n",
    "print(\"sse <= max_sse  :\", isLessThanMaxSSE)\n",
    "\n",
    "\n",
    "x_train = x_train\n",
    "y_train = out_true.reshape(len(x_train), list_layer_size[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train [[1.  0.  0. ]\n",
      " [1.  0.  0.1]\n",
      " [1.  1.  0. ]\n",
      " [1.  1.  1. ]]\n",
      "y_train [[0.1 1. ]\n",
      " [1.  0. ]\n",
      " [1.  1. ]\n",
      " [0.  0. ]]\n"
     ]
    }
   ],
   "source": [
    "x_train = x_train\n",
    "y_train = out_true.reshape(len(x_train), list_layer_size[-1])\n",
    "print(\"x_train\",x_train)\n",
    "print(\"y_train\",y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WEIGHT ORI\n",
      "|____Layer 0___|\n",
      "---B I A S---\n",
      "[0.15, 0.25]\n",
      "---N E U R O N S---\n",
      "[[0.2, 0.3], [0.35, 0.35]]\n",
      "\n",
      "\n",
      "initial error= 1.5007731751662372\n",
      "\n",
      "Activation Layers From Back to Front:  ['sigmoid']\n",
      "\n",
      "Epoch 1/10000   error=0.2699152021820066\n",
      "Epoch 2/10000   error=0.2684047727673804\n",
      "Epoch 3/10000   error=0.26690618860657817\n",
      "Epoch 4/10000   error=0.26542078162922045\n",
      "Epoch 5/10000   error=0.2639498270872234\n",
      "Epoch 6/10000   error=0.26249453446387516\n",
      "Epoch 7/10000   error=0.26105603917054965\n",
      "Epoch 8/10000   error=0.25963539517463796\n",
      "Epoch 9/10000   error=0.2582335686800107\n",
      "Epoch 10/10000   error=0.2568514329552838\n",
      "Epoch 11/10000   error=0.2554897643764058\n",
      "Epoch 12/10000   error=0.2541492397198212\n",
      "Epoch 13/10000   error=0.25283043471192623\n",
      "Epoch 14/10000   error=0.25153382381095146\n",
      "Epoch 15/10000   error=0.2502597811698778\n",
      "Epoch 16/10000   error=0.24900858270450152\n",
      "Epoch 17/10000   error=0.24778040917004246\n",
      "Epoch 18/10000   error=0.2465753501332674\n",
      "Epoch 19/10000   error=0.24539340871525117\n",
      "Epoch 20/10000   error=0.24423450697265742\n",
      "Epoch 21/10000   error=0.2430984917826191\n",
      "Epoch 22/10000   error=0.24198514109757535\n",
      "Epoch 23/10000   error=0.2408941704412859\n",
      "Epoch 24/10000   error=0.2398252395251086\n",
      "Epoch 25/10000   error=0.2387779588738472\n",
      "Epoch 26/10000   error=0.2377518963624087\n",
      "Epoch 27/10000   error=0.23674658357752296\n",
      "Epoch 28/10000   error=0.23576152193229977\n",
      "Epoch 29/10000   error=0.234796188474918\n",
      "Epoch 30/10000   error=0.23385004134584297\n",
      "Epoch 31/10000   error=0.2329225248503057\n",
      "Epoch 32/10000   error=0.23201307412410824\n",
      "Epoch 33/10000   error=0.23112111938096747\n",
      "Epoch 34/10000   error=0.23024608973848548\n",
      "Epoch 35/10000   error=0.22938741662739698\n",
      "Epoch 36/10000   error=0.22854453679501477\n",
      "Epoch 37/10000   error=0.2277168949188326\n",
      "Epoch 38/10000   error=0.22690394585012977\n",
      "Epoch 39/10000   error=0.2261051565102703\n",
      "Epoch 40/10000   error=0.2253200074643098\n",
      "Epoch 41/10000   error=0.224547994197648\n",
      "Epoch 42/10000   error=0.22378862812190478\n",
      "Epoch 43/10000   error=0.2230414373360815\n",
      "Epoch 44/10000   error=0.22230596716849405\n",
      "Epoch 45/10000   error=0.22158178052403982\n",
      "Epoch 46/10000   error=0.22086845806016503\n",
      "Epoch 47/10000   error=0.22016559821351708\n",
      "Epoch 48/10000   error=0.21947281709775787\n",
      "Epoch 49/10000   error=0.2187897482914371\n",
      "Epoch 50/10000   error=0.21811604253322325\n",
      "Epoch 51/10000   error=0.2174513673401975\n",
      "Epoch 52/10000   error=0.216795406563368\n",
      "Epoch 53/10000   error=0.21614785989306945\n",
      "Epoch 54/10000   error=0.21550844232550107\n",
      "Epoch 55/10000   error=0.21487688360032792\n",
      "Epoch 56/10000   error=0.21425292761803963\n",
      "Epoch 57/10000   error=0.21363633184461894\n",
      "Epoch 58/10000   error=0.21302686671003723\n",
      "Epoch 59/10000   error=0.2124243150061434\n",
      "Epoch 60/10000   error=0.21182847128866422\n",
      "Epoch 61/10000   error=0.21123914128726407\n",
      "Epoch 62/10000   error=0.21065614132693206\n",
      "Epoch 63/10000   error=0.2100792977633582\n",
      "Epoch 64/10000   error=0.20950844643442618\n",
      "Epoch 65/10000   error=0.20894343212948174\n",
      "Epoch 66/10000   error=0.20838410807762986\n",
      "Epoch 67/10000   error=0.20783033545595614\n",
      "Epoch 68/10000   error=0.20728198291826877\n",
      "Epoch 69/10000   error=0.2067389261446928\n",
      "Epoch 70/10000   error=0.20620104741222947\n",
      "Epoch 71/10000   error=0.20566823518620797\n",
      "Epoch 72/10000   error=0.20514038373240095\n",
      "Epoch 73/10000   error=0.20461739274944857\n",
      "Epoch 74/10000   error=0.20409916702113123\n",
      "Epoch 75/10000   error=0.20358561608795006\n",
      "Epoch 76/10000   error=0.2030766539374072\n",
      "Epoch 77/10000   error=0.2025721987123319\n",
      "Epoch 78/10000   error=0.20207217243656062\n",
      "Epoch 79/10000   error=0.20157650075725658\n",
      "Epoch 80/10000   error=0.2010851127031406\n",
      "Epoch 81/10000   error=0.20059794045789758\n",
      "Epoch 82/10000   error=0.20011491914802687\n",
      "Epoch 83/10000   error=0.1996359866444083\n",
      "Curr error < threshold. Finishing Epoch\n",
      "\n",
      "----------------------------------------------------------\n",
      "============|==========FINAL WEIGHT===========|===========\n",
      "----------------------------------------------------------\n",
      "|____Layer 0___|\n",
      "---B I A S---\n",
      "[0.14510278 0.06139219]\n",
      "---N E U R O N S---\n",
      "[[ 0.07924937  0.18389078]\n",
      " [-0.59451777 -0.76146933]]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "net.fit(x_train, y_train, learning_rate=0.1, epochs=10000, batch_size=1, err_threshold=0.2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model **Export** (`/test/model/exported/`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "act_d = {\n",
    "    \"linear\":0,\n",
    "    \"ReLU\":1,\n",
    "    \"sigmoid\":2,\n",
    "    \"softmax\":3\n",
    "}\n",
    "\n",
    "filename = input(\"Enter exported model filename (with extension): \")\n",
    "\n",
    "with open('../test/model/exported/' + filename, 'w') as file:\n",
    "    n_layer = len(net.layers)\n",
    "    file.write(str(n_layer))\n",
    "\n",
    "    for l_i in range(n_layer):\n",
    "        if not isinstance(net.layers[l_i],ActivationLayer):\n",
    "            prev_size = len(net.layers[l_i].weights[0])\n",
    "            next_size = len(net.layers[l_i].weights)\n",
    "            act_type = act_d[net.layers[l_i + 1].activation.__name__]\n",
    "            file.write(\"\\n\")\n",
    "            file.write(f\"{next_size} {prev_size} {act_type}\")\n",
    "            file.write(\"\\n\")\n",
    "            \n",
    "            b = net.layers[l_i].bias\n",
    "            for b_i in range(len(b)):\n",
    "                file.write(f\"{b[b_i]}\")\n",
    "                if b_i != len(b) - 1:\n",
    "                    file.write(\" \")\n",
    "\n",
    "            for w in net.layers[l_i].weights:\n",
    "                file.write(\"\\n\")\n",
    "                for w_i in range(len(w)):\n",
    "                    file.write(f\"{w[w_i]}\")\n",
    "                    if w_i != len(w) - 1:\n",
    "                        file.write(\" \")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iris Benchmarking with **MLPCLassifier** by _sklearn_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Model Information---\n",
      "Number of layers : 3\n",
      "Input size : 4\n",
      "Output size : 1\n",
      "\n",
      "Weights : [[[0.25, 0.25], [0.25, 0.25], [0.25, 0.25], [0.25, 0.25], [0.25, 0.25]], [[0.0], [0.25], [0.25]]]\n",
      "Previous layer size : [4, 2]\n",
      "Current layer size : [2, 1]\n",
      "Activation function : [2, 2]\n",
      "\n",
      "---Prediction Information---\n",
      "Input data : [[1.  5.1 3.5 1.4 0.2]]\n",
      "Output prediction : [0.61570037]\n",
      "Output true : [0.]\n",
      "SSE  :  0.189543474844611\n",
      "sse <= max_sse  : False\n"
     ]
    }
   ],
   "source": [
    "MODEL_FOLDER = '../test/model/'\n",
    "INPUT_FOLDER = '../test/input/'\n",
    "OUTPUT_FOLDER = '../test/output/'\n",
    "# read file from test folder\n",
    "def read_file(folder_path, file_name):\n",
    "    with open(folder_path + file_name, 'r') as file:\n",
    "        data = [i.rstrip(\"\\n\") for i in file.readlines()]\n",
    "    return data\n",
    "\n",
    "filename = 'iris.txt' # Do not change this\n",
    "data = read_file(MODEL_FOLDER, filename)\n",
    "input_data = read_file(INPUT_FOLDER, filename)\n",
    "\n",
    "try: \n",
    "    output_data = read_file(OUTPUT_FOLDER, filename)\n",
    "except:\n",
    "    output_data = None\n",
    "\n",
    "# Load model building blocks\n",
    "weights, list_prev_size, list_layer_size, list_activation = model_load(data)\n",
    "\n",
    "# Build NN Model\n",
    "act = {0: linear, 1: ReLU, 2: sigmoid, 3: softmax}\n",
    "net = NeuralNetwork()\n",
    "for i in range (int(data[0]) - 1):\n",
    "    net.add(ConnectedLayer(list_prev_size[i], list_layer_size[i], weights[i]))\n",
    "    net.add(ActivationLayer(act[list_activation[i]]))\n",
    "\n",
    "# Generate x_train\n",
    "x_train = generate_x_train(input_data)\n",
    "out, complete_out = predict_output(x_train, net)\n",
    "\n",
    "# Print the information, complete with brief verbose\n",
    "print('\\n---Prediction Information---')\n",
    "print('Input data :', x_train)\n",
    "\n",
    "# Compute out error\n",
    "out_pred, out_true, sse_err, isLessThanMaxSSE = compute_out_and_errors(output_data, out)\n",
    "\n",
    "print('Output prediction :', out_pred)\n",
    "print('Output true :', out_true)\n",
    "print('SSE  : ', sse_err)\n",
    "print(\"sse <= max_sse  :\", isLessThanMaxSSE)\n",
    "\n",
    "\n",
    "x_train = x_train\n",
    "y_train = out_true.reshape(len(x_train), list_layer_size[-1])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Restructure train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load iris import datasets first\n",
    "from sklearn import datasets\n",
    "iris = datasets.load_iris()\n",
    "\n",
    "x_train = np.insert(iris.data, 0, 1, axis=1)\n",
    "y_train = iris.target.reshape(-1,1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train with created NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WEIGHT ORI\n",
      "|____Layer 0___|\n",
      "---B I A S---\n",
      "[0.14880048 0.14880048]\n",
      "---N E U R O N S---\n",
      "[[-0.07693405 -0.07693405]\n",
      " [-0.18598671 -0.18598671]\n",
      " [ 0.61389816  0.61389816]\n",
      " [ 0.43554048  0.43554048]]\n",
      "\n",
      "\n",
      "|____Layer 1___|\n",
      "---B I A S---\n",
      "[0.63107422]\n",
      "---N E U R O N S---\n",
      "[[1.42347946]\n",
      " [1.42347946]]\n",
      "\n",
      "\n",
      "initial error= 124.03767831601542\n",
      "\n",
      "Activation Layers From Back to Front:  ['sigmoid', 'sigmoid']\n",
      "\n",
      "Epoch 1/5   error=0.2929304390443457\n",
      "Epoch 2/5   error=0.2688820975911129\n",
      "Epoch 3/5   error=0.24434109492339615\n",
      "Epoch 4/5   error=0.22402629909387983\n",
      "Epoch 5/5   error=0.2093357328322198\n",
      "\n",
      "----------------------------------------------------------\n",
      "============|==========FINAL WEIGHT===========|===========\n",
      "----------------------------------------------------------\n",
      "|____Layer 0___|\n",
      "---B I A S---\n",
      "[0.03872439 0.03872439]\n",
      "---N E U R O N S---\n",
      "[[-0.32602797 -0.32602797]\n",
      " [-0.67715882 -0.67715882]\n",
      " [ 1.20281818  1.20281818]\n",
      " [ 0.7160292   0.7160292 ]]\n",
      "\n",
      "\n",
      "|____Layer 1___|\n",
      "---B I A S---\n",
      "[-0.76912169]\n",
      "---N E U R O N S---\n",
      "[[2.29779165]\n",
      " [2.29779165]]\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\AppData\\Local\\Temp/ipykernel_26304/574847519.py:103: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  prev_out = uniform2D(np.array(complete_out)[k, -2]).reshape(-1,1)\n",
      "C:\\Users\\HP\\AppData\\Local\\Temp/ipykernel_26304/574847519.py:139: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  succ_out = np.array(complete_out)[k, reversed_i].reshape(-1,1)\n"
     ]
    }
   ],
   "source": [
    "net.fit(x_train, y_train, learning_rate=0.1, epochs=5, batch_size=1, err_threshold=0.2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train with MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights [array([[-0.16583291,  0.77798758],\n",
      "       [-0.99902979,  0.18348124],\n",
      "       [-0.70596426, -1.27022147],\n",
      "       [-0.62701422, -0.53722596]]), array([[-0.17690465,  0.40549464, -0.64703246],\n",
      "       [ 1.38531949, -1.09025746, -0.12873798]])]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (5) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# train with mlpclassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "clf = MLPClassifier(solver='sgd', hidden_layer_sizes=(2), random_state=1, max_iter=5, batch_size=1, activation='relu')\n",
    "clf.fit(iris.data, iris.target)\n",
    "\n",
    "# print the weights\n",
    "print(\"weights\", clf.coefs_)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
