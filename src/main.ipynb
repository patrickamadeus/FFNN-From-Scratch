{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **NN** Implementation (Forward + Backward Propagation)\n",
    "1. Add input in /test/input and model in /test/model with .txt format, makesure they both have the same filename.\n",
    "2. Input the filename for both the input and the model, ensuring that they have the same filename.\n",
    "3. Scroll down and click on `./tmp/network.html`. This will redirect you to network.html, which you can then open the visualization using a live server.\n",
    "\n",
    "#### Made by:\n",
    "- Samuel Christoper Swandi - 13520075\n",
    "- Grace Claudia - 13520078\n",
    "- Ubaidillah Ariq Prathama - 13520085\n",
    "- Patrick Amadeus Irawan - 13520109\n",
    "\n",
    "------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "List of Content\n",
    "\n",
    "1. [Library & Dependencies](#library)\n",
    "2. [Helper Function](#helper)\n",
    "3. [Neural Network Visualization](#visualization)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Library & Dependencies <a name=\"library\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m pip install pyvis==0.3.2\n",
    "!python3 -m pip install networkx==2.6.3\n",
    "!python3 -m pip install numpy==1.21.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pandas import DataFrame\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "import os\n",
    "\n",
    "from pyvis.network import Network"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Class**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Connected Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConnectedLayer:\n",
    "    def __init__(self, input_size, output_size, weights):\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.weights = weights[1:]\n",
    "        self.bias = weights[0]\n",
    "\n",
    "    def forward(self, input):\n",
    "        self.input = input\n",
    "        return np.dot(input, self.weights) + self.bias"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Activation Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActivationLayer:\n",
    "    def __init__(self, activation):\n",
    "        self.activation = activation\n",
    "    \n",
    "    def forward(self, input):\n",
    "        self.input = input\n",
    "        return self.activation(input)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 526,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import ceil \n",
    "\n",
    "class NeuralNetwork:\n",
    "    def __init__(self):\n",
    "        self.layers = []\n",
    "\n",
    "    def add(self, layer):\n",
    "        self.layers.append(layer)\n",
    "    \n",
    "    def predict(self, x):\n",
    "        complete_result = []\n",
    "        output = x[:, 1:]\n",
    "\n",
    "        for layer in self.layers:\n",
    "            output = layer.forward(output)\n",
    "            if isinstance(layer, ActivationLayer):\n",
    "                complete_result.append(output)\n",
    "\n",
    "        return complete_result[-1] , complete_result\n",
    "    \n",
    "    def print_weight(self):\n",
    "        i = 0\n",
    "        for layer in self.layers:\n",
    "            if not isinstance(layer, ActivationLayer):\n",
    "                print(\"|____Layer {}___|\".format(i))\n",
    "                print(\"---B I A S---\")\n",
    "                print(layer.bias)\n",
    "                print(\"---N E U R O N S---\")\n",
    "                print(layer.weights)\n",
    "                i+=1\n",
    "            print()\n",
    "\n",
    "    def fit(self, x_train, y_train, learning_rate=0.1, epochs = 5, batch_size = 2, err_threshold = 0.01):\n",
    "        # batch size validation\n",
    "        if batch_size > len(x_train) or batch_size <= 0:\n",
    "            print(\"Batch size {} invalid.\".format(batch_size))\n",
    "            print(\"Batch size range [{}...{}]\".format(1,len(x_train)))\n",
    "            return\n",
    "\n",
    "        print(\"WEIGHT ORI\")\n",
    "        self.print_weight()\n",
    "        print(\"initial error=\", sse(predict_output(x_train, self)[0], y_train))\n",
    "        print(\"X_train\")\n",
    "        print(x_train)\n",
    "        print(\"Pred\",predict_output(x_train, self)[0])\n",
    "        print(\"Ori\", y_train)\n",
    "        print()\n",
    "        # training loop\n",
    "        for i in range(epochs):  # Epoch Loop\n",
    "            print(\"- - - - - - - - - - - - - - - - EPOCH {} - - - - - - - - - - - - - - - -\".format(i+1))\n",
    "            # err = 0\n",
    "            start_i = 0\n",
    "            end_i = batch_size\n",
    "\n",
    "            for j in range(ceil(len(x_train) / batch_size)):  # Mini Batch Loop\n",
    "                print(\"============BATCH {}==============\".format(j + 1))\n",
    "                batch_x_train = x_train[start_i:end_i]\n",
    "                batch_y_train = y_train[start_i:end_i]\n",
    "\n",
    "                # FORWARD PROPAGATION\n",
    "                batch_y_pred, complete_out = predict_output(batch_x_train, self)\n",
    "                batch_y_pred = np.array(batch_y_pred)\n",
    "                # err += sse(batch_y_pred, batch_y_train)\n",
    "\n",
    "                # BACKWARD PROPAGATION\n",
    "                out_grad = None\n",
    "                hid_grad = []\n",
    "\n",
    "                for k in range(len(batch_x_train)):  # Single Sample Backpropagation Loop\n",
    "                    sample_y_pred = batch_y_pred[k]\n",
    "                    sample_y_train = batch_y_train[k]\n",
    "\n",
    "                    #-------------- Output Gradient --------------#\n",
    "                    prev_out = uniform2D(np.array(complete_out)[k, -2]).reshape(-1,1)\n",
    "\n",
    "                    # Duplicate predecessor to match output layer neuron size & append bias\n",
    "                    succ_size = len(complete_out[0][-1])\n",
    "                    sample_pred_out = np.hstack([prev_out] * succ_size)\n",
    "                    sample_pred_out = np.vstack([np.ones(succ_size),sample_pred_out])\n",
    "\n",
    "                    # print(\"\\n--Output Materials--\")\n",
    "                    # print(\"sample_y_pred\")\n",
    "                    # print(sample_y_pred)\n",
    "                    # print(\"sample_y_train\")\n",
    "                    # print(sample_y_train)\n",
    "                    # print(\"sample_pred_out\")\n",
    "                    # print(sample_pred_out)\n",
    "\n",
    "                    dE_dW, succ_dE_dNet = grad_output(sample_y_pred, sample_y_train, sample_pred_out, 2) # TODO CHANGE NOT SIGMOID\n",
    "                    if out_grad is None:\n",
    "                        out_grad = dE_dW\n",
    "                    else:\n",
    "                        out_grad += dE_dW\n",
    "\n",
    "                    \n",
    "                    # -------------- Hidden Gradient --------------#\n",
    "                    # TODO : INCREMENTAL LAYER IMPLEMENTATION, STILL IMPLEMENTED DIRECTLY TO INPUT, cant support more than 3 layer\n",
    "                    prev_out = np.array(batch_x_train[k][1:]).reshape(-1,1)  # cut the bias first\n",
    "                    # prev_out = uniform2D(np.array(complete_out)[k, -2]).reshape(-1,1)\n",
    "                    \n",
    "                    succ_out = np.array(complete_out)[k, -2].reshape(-1,1)\n",
    "                    succ_weight = np.array(self.layers[-2].weights)\n",
    "                    # Duplicate predecessor to match succ layer neuron size & append bias\n",
    "                    succ_size = len(complete_out[0][-2])\n",
    "                    sample_pred_out = np.hstack([prev_out] * succ_size)\n",
    "                    sample_pred_out = np.vstack([np.ones(succ_size),sample_pred_out])\n",
    "\n",
    "                    # print(\"\\n--Hidden Materials--\")\n",
    "                    # print(\"sample_pred_out\")\n",
    "                    # print(sample_pred_out)\n",
    "                    # print(\"succ_out\")\n",
    "                    # print(succ_out)\n",
    "                    # print(\"succ_dE_dNet\")\n",
    "                    # print(succ_dE_dNet)\n",
    "                    # print(\"succ_weight\")\n",
    "                    # print(succ_weight)\n",
    "\n",
    "                    dEtotal_dW, succ_dEtotal_dNet = grad_hidden(succ_dE_dNet, succ_weight, succ_out, sample_pred_out, 2) # TODO CHANGE NOT SIGMOID\n",
    "                    hid_grad.append(dEtotal_dW)\n",
    "\n",
    "\n",
    "                out_delta = -learning_rate * out_grad\n",
    "                hid_delta = -learning_rate * np.array(hid_grad)\n",
    "\n",
    "                # reverse hidden delta\n",
    "                deltas = (hid_delta[::-1].tolist() + [out_delta.tolist()])\n",
    "                for d_i in range(len(deltas)):\n",
    "                    deltas[d_i] = np.array(deltas[d_i])\n",
    "                # print(\"---All Delta---\")\n",
    "                # print(deltas)\n",
    "\n",
    "                # Update weight\n",
    "                d_i = 0\n",
    "                for layer in self.layers:\n",
    "                    if not isinstance(layer, ActivationLayer):\n",
    "                        layer.bias += deltas[d_i][0]\n",
    "                        layer.weights += deltas[d_i][1:]\n",
    "                        d_i+=1  \n",
    "\n",
    "                # Increment next batch\n",
    "                start_i += batch_size\n",
    "                end_i += batch_size\n",
    "                print()\n",
    "\n",
    "                self.print_weight()\n",
    "\n",
    "\n",
    "            # calculate average error on all samples\n",
    "            print(\"X_train\")\n",
    "            print(x_train)\n",
    "            print(\"Pred\")\n",
    "            print(predict_output(x_train, self)[0])\n",
    "            print(\"Ori\")\n",
    "            print(y_train)\n",
    "            err = sse(predict_output(x_train, self)[0], y_train)\n",
    "\n",
    "            print(\"Epoch {}/{}   error={}\".format(i+1,epochs, err))\n",
    "            if err < err_threshold:\n",
    "                print(\"Curr error < threshold. Finishing Epoch\")\n",
    "                break\n",
    "        print(\"\\n----------------------------------------------------------\")\n",
    "        print(\"============|==========FINAL WEIGHT===========|===========\")\n",
    "        print(\"----------------------------------------------------------\")\n",
    "\n",
    "        self.print_weight()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def uniform2D(arr):\n",
    "    return np.array([np.array(i) for i in arr])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Helper** Function <a class=\"anchor\" id=\"helper\"></a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Activation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear(net):\n",
    "    return net\n",
    "\n",
    "def ReLU(net):\n",
    "    return np.maximum(0,net)\n",
    "\n",
    "def sigmoid(net):\n",
    "    return 1/(1+np.exp(-net))\n",
    "\n",
    "def softmax(net):\n",
    "    res = []\n",
    "    for sample in net:\n",
    "        res.append(np.exp(sample)/np.sum(np.exp(sample)))\n",
    "    return np.array(res)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse(y_true, y_pred):\n",
    "    return np.mean(np.square(y_true - y_pred))\n",
    "\n",
    "def sse(y_true, y_pred):\n",
    "    return np.sum(np.square(y_true - y_pred))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Loading Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_load(data):\n",
    "    '''\n",
    "    Function to load model from file\n",
    "    INPUT :     data -> data from model file\n",
    "\n",
    "    OUTPUT :    weights -> weights of each model neuron\n",
    "                list_prev_size -> list of previous layer size\n",
    "                list_layer_size -> list of current layer size\n",
    "                list_activation -> list of activation function\n",
    "    '''\n",
    "\n",
    "    idx = 1\n",
    "    weights,list_prev_size,list_layer_size,list_activation = [],[],[],[]\n",
    "\n",
    "    for i in range(int(data[0]) - 1):\n",
    "        # Loading size & act function\n",
    "        prev_size, layer_size, activation = [int(i) for i in data[idx].split()]\n",
    "        list_prev_size.append(prev_size)\n",
    "        list_layer_size.append(layer_size)\n",
    "        list_activation.append(activation)\n",
    "        \n",
    "        # Loading weights\n",
    "        idx += 1\n",
    "        weight = []\n",
    "        for j in range(prev_size + 1):\n",
    "            weight.append([float(i) for i in data[idx].split()])\n",
    "            idx += 1\n",
    "        weights.append(weight)\n",
    "    \n",
    "    # Verbose Information\n",
    "    print('---Model Information---')\n",
    "    print('Number of layers :', len(list_prev_size) + 1)\n",
    "    print('Input size :', list_prev_size[0])\n",
    "    print('Output size :', list_layer_size[-1])\n",
    "\n",
    "    print()\n",
    "    print('Weights :', weights)\n",
    "    print('Previous layer size :', list_prev_size)\n",
    "    print('Current layer size :', list_layer_size)\n",
    "    print('Activation function :', list_activation)\n",
    "\n",
    "    return weights, list_prev_size, list_layer_size, list_activation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Input Loading Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_x_train(input_data):\n",
    "    x_train = []\n",
    "    # Parse each line\n",
    "    for i in range(len(input_data)):\n",
    "        x_train.append([float(i) for i in input_data[i].split()])\n",
    "\n",
    "    return np.array(x_train)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predict Output Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_output(x, net):\n",
    "    '''\n",
    "    Function to predict output from input data\n",
    "    INPUT :     x-> input data\n",
    "                net -> NeuralNetwork object\n",
    "\n",
    "    OUTPUT :    out -> output of the model\n",
    "                complete_out -> complete output of the model, visualization purpose\n",
    "    '''\n",
    "    # Predict output\n",
    "    out, complete_out = net.predict(x)\n",
    "\n",
    "    # Gather complete output\n",
    "    n_complete_out = []\n",
    "    for i in range(len(complete_out[0])):\n",
    "        n_complete_out.append([complete_out[0][i], complete_out[-1][i]])\n",
    "\n",
    "    complete_out = n_complete_out\n",
    "    return out, complete_out"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SSE Errors Function + microhelper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_out_and_errors(output_data, out):\n",
    "    '''\n",
    "    Function to calculate output error\n",
    "    INPUT :     output_data -> output data from output file\n",
    "                out -> output of the model, obtained from predict_output function\n",
    "\n",
    "    OUTPUT :    out_pred -> output of the model (flattened)\n",
    "                out_true -> output from output_data (flattened)\n",
    "                sse_error -> sum squared error\n",
    "                sse_error <= max_sse -> boolean value, True if sse_error <= max_sse, False otherwise\n",
    "    '''\n",
    "    # Assign y_train from output_data\n",
    "    parsed_output = [[float(j) for j in i.split()] for i in output_data]\n",
    "    out_pred = out.flatten()\n",
    "    out_true = np.array(parsed_output[:-1]).flatten()\n",
    "    max_sse = parsed_output[-1][0]\n",
    "\n",
    "    return out_pred, out_true, sse(out_true, out_pred), sse(out_true, out_pred) <= max_sse"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Load** Section"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entrypoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 473,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_FOLDER = '../test/model/'\n",
    "INPUT_FOLDER = '../test/input/'\n",
    "OUTPUT_FOLDER = '../test/output/'\n",
    "# read file from test folder\n",
    "def read_file(folder_path, file_name):\n",
    "    with open(folder_path + file_name, 'r') as file:\n",
    "        data = [i.rstrip(\"\\n\") for i in file.readlines()]\n",
    "    return data\n",
    "\n",
    "filename = input('Enter test case name (with extension): ')\n",
    "data = read_file(MODEL_FOLDER, filename)\n",
    "input_data = read_file(INPUT_FOLDER, filename)\n",
    "\n",
    "try: \n",
    "    output_data = read_file(OUTPUT_FOLDER, filename)\n",
    "except:\n",
    "    output_data = None"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model_load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 474,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Model Information---\n",
      "Number of layers : 3\n",
      "Input size : 2\n",
      "Output size : 1\n",
      "\n",
      "Weights : [[[0.25, 0.25], [0.25, 0.25], [0.25, 0.25]], [[0.0], [0.25], [0.25]]]\n",
      "Previous layer size : [2, 2]\n",
      "Current layer size : [2, 1]\n",
      "Activation function : [2, 2]\n"
     ]
    }
   ],
   "source": [
    "weights, list_prev_size, list_layer_size, list_activation = model_load(data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 475,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build NN model\n",
    "act = {0: linear, 1: ReLU, 2: sigmoid, 3: softmax}\n",
    "net = NeuralNetwork()\n",
    "for i in range (int(data[0]) - 1):\n",
    "    net.add(ConnectedLayer(list_prev_size[i], list_layer_size[i], weights[i]))\n",
    "    net.add(ActivationLayer(act[list_activation[i]]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Predict** Section"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare x_train & predict output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 476,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Prediction Information---\n",
      "Input data : [[ 1.  1.  1.]\n",
      " [ 1.  1. -1.]\n",
      " [ 1. -1.  1.]\n",
      " [ 1. -1. -1.]]\n",
      "Output data : [[0.58409077]\n",
      " [0.569813  ]\n",
      " [0.569813  ]\n",
      " [0.55451042]]\n"
     ]
    }
   ],
   "source": [
    "x_train = generate_x_train(input_data)\n",
    "out, complete_out = predict_output(x_train, net)\n",
    "\n",
    "# Print the information, complete with brief verbose\n",
    "print('---Prediction Information---')\n",
    "print('Input data :', x_train)\n",
    "print('Output data :', out)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute Error with **Sum Squared Error (SSE)**\n",
    "\n",
    "Sum Squared Error (SSE) is a mathematical function used in statistics and machine learning to measure the difference between predicted and actual values. It is commonly used as a cost function in various optimization algorithms, such as gradient descent.\n",
    "\n",
    "The SSE is calculated by taking the difference between each predicted value and its corresponding actual value, squaring the difference, and then summing all of the squared differences:\n",
    "\n",
    "$$SSE = \\sum_{i=1}^{n}(y_i - \\hat{y}_i)^2$$\n",
    "\n",
    "Where:\n",
    "- $n$ is the number of data points\n",
    "- $y_i$ is the actual value of the i-th data point\n",
    "- $\\hat{y}_i$ is the predicted value of the i-th data point\n",
    "\n",
    "The SSE gives an indication of how well the model fits the data. A lower SSE indicates that the model is a better fit for the data.\n",
    "\n",
    "In machine learning, the SSE is often used as a cost function to be minimized during training of a model. The goal is to find the set of model parameters that minimizes the SSE, thus improving the accuracy of the model's predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 477,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output prediction : [0.58409077 0.569813   0.569813   0.55451042]\n",
      "Output true : [ 1. -1. -1.  1.]\n",
      "SSE  :  5.300067181259587\n",
      "sse <= max_sse  : False\n"
     ]
    }
   ],
   "source": [
    "out_pred, out_true, sse_err, isLessThanMaxSSE = compute_out_and_errors(output_data, out)\n",
    "\n",
    "print('Output prediction :', out_pred)\n",
    "print('Output true :', out_true)\n",
    "print('SSE  : ', sse_err)\n",
    "print(\"sse <= max_sse  :\", isLessThanMaxSSE)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network Visualization with **Pyvis**\n",
    "\n",
    "Pyvis is a Python library that provides an easy-to-use interface for visualizing complex networks, including neural networks. Implementation of the visualization is enlisted below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_network(data, complete_out) -> Network:\n",
    "    n = int(data[0])\n",
    "\n",
    "    # Constant\n",
    "    XSTEP, YSTEP, SIZE = 300, 300, 10\n",
    "\n",
    "    # Nodes\n",
    "    nodes = []\n",
    "    node_i = 1\n",
    "\n",
    "    # Nodes Value\n",
    "    value = []\n",
    "    x_val = 0\n",
    "    y_val = 0\n",
    "\n",
    "    # Position\n",
    "    x = []\n",
    "    y = []\n",
    "\n",
    "    # Styling + Text\n",
    "    label = []\n",
    "    color = []\n",
    "    edge = []\n",
    "    title = []\n",
    "\n",
    "    # Indexing\n",
    "    src_idx = 0\n",
    "    idx = 1\n",
    "\n",
    "    for i_layer in range(n - 1):\n",
    "        n_curr, n_next, _ = [int(i) for i in data[idx].split()]\n",
    "\n",
    "        if i_layer == 0:  # means that this is the first layer, hence construct input\n",
    "            for i_node in range(n_curr + 1):\n",
    "                if i_node == 0:  # bias\n",
    "                    color.append(\"#dd4b39\")\n",
    "                    label.append(\"Input[bias]\")\n",
    "                    temp = \"\"\n",
    "                    for i in range (len(input_data)):\n",
    "                        temp += str(x_train[i][0])\n",
    "                        if i < len(input_data) - 1:\n",
    "                            temp += \", \"\n",
    "                    title.append(temp)\n",
    "                else:\n",
    "                    color.append(\"#162347\")\n",
    "                    label.append(\"Input[{}]\".format(i_node))\n",
    "                    temp = \"\"\n",
    "                    for i in range (len(input_data)):\n",
    "                        temp += str(x_train[i][i_node])\n",
    "                        if i < len(input_data) - 1:\n",
    "                            temp += \", \"\n",
    "                    title.append(temp)\n",
    "                value.append(SIZE)\n",
    "                x.append(x_val)\n",
    "                y.append(y_val)\n",
    "                y_val += YSTEP\n",
    "                nodes.append(node_i)\n",
    "                node_i += 1\n",
    "            x_val += XSTEP\n",
    "\n",
    "        y_val = 0\n",
    "        # always construct the next layer\n",
    "        for i_node in range(n_next + 1):\n",
    "            if i_node == 0:\n",
    "                if i_layer == n - 2:\n",
    "                    continue\n",
    "                color.append(\"#dd4b39\")\n",
    "                label.append(\"HL{}[bias]\".format(i_layer + 1))\n",
    "                temp = \"\"\n",
    "                for i in range (len(input_data)):\n",
    "                    temp +=  \"1\"\n",
    "                    if i < len(input_data) - 1:\n",
    "                        temp += \", \"\n",
    "                title.append(temp)\n",
    "            else:\n",
    "                color.append(\"#162347\")\n",
    "                if i_layer == n - 2:\n",
    "                    label.append(\"Output[{}]\".format(i_node))\n",
    "                else:\n",
    "                    label.append(\"HL{}[{}]\".format(i_layer + 1, i_node))\n",
    "                temp = \"\"\n",
    "                for i in range (len(input_data)):\n",
    "                    temp += str(complete_out[i][i_layer][i_node - 1])\n",
    "                    if i < len(input_data) - 1:\n",
    "                        temp += \", \"\n",
    "                title.append(temp)\n",
    "            value.append(SIZE)\n",
    "            x.append(x_val)\n",
    "            y.append(y_val)\n",
    "            y_val += YSTEP\n",
    "            nodes.append(node_i)\n",
    "            node_i += 1\n",
    "        x_val += XSTEP\n",
    "\n",
    "        idx += 1\n",
    "        for origin in range(n_curr + 1):\n",
    "            dst_idx = -1\n",
    "            for w in reversed(data[idx].split()):\n",
    "                edge.append((nodes[src_idx], nodes[dst_idx], w))\n",
    "                dst_idx -= 1\n",
    "            src_idx += 1\n",
    "            idx += 1\n",
    "\n",
    "    g = Network(notebook=True, cdn_resources=\"remote\")\n",
    "    g.add_nodes(nodes,title = title,value = value,x=x,y=y,label = label,color = color)\n",
    "\n",
    "    for e in edge:\n",
    "        g.add_edge(e[0], e[1], title = e[2], color=\"#162347\")\n",
    "\n",
    "    for n in g.nodes:\n",
    "        n.update({'physics': False})\n",
    "    \n",
    "    return g"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize using Helper Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = visualize_network(data, complete_out)\n",
    "g.show(\"./tmp/network.html\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backpropagation Section"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit and Train NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 529,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WEIGHT ORI\n",
      "|____Layer 0___|\n",
      "---B I A S---\n",
      "[0.25, 0.25]\n",
      "---N E U R O N S---\n",
      "[[0.25, 0.25], [0.25, 0.25]]\n",
      "\n",
      "\n",
      "|____Layer 1___|\n",
      "---B I A S---\n",
      "[0.0]\n",
      "---N E U R O N S---\n",
      "[[0.25], [0.25]]\n",
      "\n",
      "\n",
      "initial error= 5.300067181259587\n",
      "X_train\n",
      "[[ 1.  1.  1.]\n",
      " [ 1.  1. -1.]\n",
      " [ 1. -1.  1.]\n",
      " [ 1. -1. -1.]]\n",
      "Pred [[0.58409077]\n",
      " [0.569813  ]\n",
      " [0.569813  ]\n",
      " [0.55451042]]\n",
      "Ori [[ 1.]\n",
      " [-1.]\n",
      " [-1.]\n",
      " [ 1.]]\n",
      "\n",
      "- - - - - - - - - - - - - - - - EPOCH 1 - - - - - - - - - - - - - - - -\n",
      "============BATCH 1==============\n",
      "\n",
      "|____Layer 0___|\n",
      "---B I A S---\n",
      "[0.25067717 0.25067717]\n",
      "---N E U R O N S---\n",
      "[[0.24932283 0.24932283]\n",
      " [0.24932283 0.24932283]]\n",
      "\n",
      "\n",
      "|____Layer 1___|\n",
      "---B I A S---\n",
      "[-0.00236782 -0.00236782]\n",
      "---N E U R O N S---\n",
      "[[0.25236782 0.25236782]\n",
      " [0.24763218 0.24763218]]\n",
      "\n",
      "\n",
      "X_train\n",
      "[[ 1.  1.  1.]\n",
      " [ 1.  1. -1.]\n",
      " [ 1. -1.  1.]\n",
      " [ 1. -1. -1.]]\n",
      "Pred\n",
      "[[0.58349751 0.58349751]\n",
      " [0.56925293 0.56925293]\n",
      " [0.56925293 0.56925293]\n",
      " [0.55398721 0.55398721]]\n",
      "Ori\n",
      "[[ 1.]\n",
      " [-1.]\n",
      " [-1.]\n",
      " [ 1.]]\n",
      "Epoch 1/5   error=10.595022445451665\n",
      "- - - - - - - - - - - - - - - - EPOCH 2 - - - - - - - - - - - - - - - -\n",
      "============BATCH 1==============\n",
      "\n",
      "|____Layer 0___|\n",
      "---B I A S---\n",
      "[0.2520466 0.2520209]\n",
      "---N E U R O N S---\n",
      "[[0.2479534 0.2479791]\n",
      " [0.2479534 0.2479791]]\n",
      "\n",
      "\n",
      "|____Layer 1___|\n",
      "---B I A S---\n",
      "[-0.00714773 -0.00705804]\n",
      "---N E U R O N S---\n",
      "[[0.25714773 0.25705804]\n",
      " [0.24285227 0.24294196]]\n",
      "\n",
      "\n",
      "X_train\n",
      "[[ 1.  1.  1.]\n",
      " [ 1.  1. -1.]\n",
      " [ 1. -1.  1.]\n",
      " [ 1. -1. -1.]]\n",
      "Pred\n",
      "[[0.58229942 0.58232124]\n",
      " [0.56812145 0.56814346]\n",
      " [0.56812145 0.56814346]\n",
      " [0.55292977 0.55295194]]\n",
      "Ori\n",
      "[[ 1.]\n",
      " [-1.]\n",
      " [-1.]\n",
      " [ 1.]]\n",
      "Epoch 2/5   error=10.584810700324898\n",
      "- - - - - - - - - - - - - - - - EPOCH 3 - - - - - - - - - - - - - - - -\n",
      "============BATCH 1==============\n",
      "\n",
      "|____Layer 0___|\n",
      "---B I A S---\n",
      "[0.25344633 0.25334328]\n",
      "---N E U R O N S---\n",
      "[[0.24655367 0.24665672]\n",
      " [0.24655367 0.24665672]]\n",
      "\n",
      "\n",
      "|____Layer 1___|\n",
      "---B I A S---\n",
      "[-0.01201607 -0.0116574 ]\n",
      "---N E U R O N S---\n",
      "[[0.26201607 0.2616574 ]\n",
      " [0.23798393 0.2383426 ]]\n",
      "\n",
      "\n",
      "X_train\n",
      "[[ 1.  1.  1.]\n",
      " [ 1.  1. -1.]\n",
      " [ 1. -1.  1.]\n",
      " [ 1. -1. -1.]]\n",
      "Pred\n",
      "[[0.58107865 0.58116596]\n",
      " [0.56696774 0.5670558 ]\n",
      " [0.56696774 0.5670558 ]\n",
      " [0.55185061 0.55193931]]\n",
      "Ori\n",
      "[[ 1.]\n",
      " [-1.]\n",
      " [-1.]\n",
      " [ 1.]]\n",
      "Epoch 3/5   error=10.57461685711907\n",
      "- - - - - - - - - - - - - - - - EPOCH 4 - - - - - - - - - - - - - - - -\n",
      "============BATCH 1==============\n",
      "\n",
      "|____Layer 0___|\n",
      "---B I A S---\n",
      "[0.25487652 0.25464412]\n",
      "---N E U R O N S---\n",
      "[[0.24512348 0.24535588]\n",
      " [0.24512348 0.24535588]]\n",
      "\n",
      "\n",
      "|____Layer 1___|\n",
      "---B I A S---\n",
      "[-0.01697269 -0.01616593]\n",
      "---N E U R O N S---\n",
      "[[0.26697269 0.26616593]\n",
      " [0.23302731 0.23383407]]\n",
      "\n",
      "\n",
      "X_train\n",
      "[[ 1.  1.  1.]\n",
      " [ 1.  1. -1.]\n",
      " [ 1. -1.  1.]\n",
      " [ 1. -1. -1.]]\n",
      "Pred\n",
      "[[0.57983516 0.5800317 ]\n",
      " [0.56579184 0.56599002]\n",
      " [0.56579184 0.56599002]\n",
      " [0.55074989 0.55094946]]\n",
      "Ori\n",
      "[[ 1.]\n",
      " [-1.]\n",
      " [-1.]\n",
      " [ 1.]]\n",
      "Epoch 4/5   error=10.564441572950468\n",
      "- - - - - - - - - - - - - - - - EPOCH 5 - - - - - - - - - - - - - - - -\n",
      "============BATCH 1==============\n",
      "\n",
      "|____Layer 0___|\n",
      "---B I A S---\n",
      "[0.25633734 0.25592323]\n",
      "---N E U R O N S---\n",
      "[[0.24366266 0.24407677]\n",
      " [0.24366266 0.24407677]]\n",
      "\n",
      "\n",
      "|____Layer 1___|\n",
      "---B I A S---\n",
      "[-0.02201743 -0.02058367]\n",
      "---N E U R O N S---\n",
      "[[0.27201743 0.27058367]\n",
      " [0.22798257 0.22941633]]\n",
      "\n",
      "\n",
      "X_train\n",
      "[[ 1.  1.  1.]\n",
      " [ 1.  1. -1.]\n",
      " [ 1. -1.  1.]\n",
      " [ 1. -1. -1.]]\n",
      "Pred\n",
      "[[0.57856889 0.57891848]\n",
      " [0.56459382 0.56494621]\n",
      " [0.56459382 0.56494621]\n",
      " [0.54962778 0.54998256]]\n",
      "Ori\n",
      "[[ 1.]\n",
      " [-1.]\n",
      " [-1.]\n",
      " [ 1.]]\n",
      "Epoch 5/5   error=10.554285522770899\n",
      "\n",
      "----------------------------------------------------------\n",
      "============|==========FINAL WEIGHT===========|===========\n",
      "----------------------------------------------------------\n",
      "|____Layer 0___|\n",
      "---B I A S---\n",
      "[0.25633734 0.25592323]\n",
      "---N E U R O N S---\n",
      "[[0.24366266 0.24407677]\n",
      " [0.24366266 0.24407677]]\n",
      "\n",
      "\n",
      "|____Layer 1___|\n",
      "---B I A S---\n",
      "[-0.02201743 -0.02058367]\n",
      "---N E U R O N S---\n",
      "[[0.27201743 0.27058367]\n",
      " [0.22798257 0.22941633]]\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\AppData\\Local\\Temp/ipykernel_39484/4256562812.py:74: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  prev_out = uniform2D(np.array(complete_out)[k, -2]).reshape(-1,1)\n",
      "C:\\Users\\HP\\AppData\\Local\\Temp/ipykernel_39484/4256562812.py:101: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  succ_out = np.array(complete_out)[k, -2].reshape(-1,1)\n"
     ]
    }
   ],
   "source": [
    "# Build NN model\n",
    "act = {0: linear, 1: ReLU, 2: sigmoid, 3: softmax}\n",
    "net = NeuralNetwork()\n",
    "for i in range (int(data[0]) - 1):\n",
    "    net.add(ConnectedLayer(list_prev_size[i], list_layer_size[i], weights[i]))\n",
    "    net.add(ActivationLayer(act[list_activation[i]]))\n",
    "\n",
    "# Train NN\n",
    "x_train = x_train\n",
    "y_train = out_true.reshape(len(x_train), list_layer_size[-1])\n",
    "\n",
    "net.fit(x_train, y_train, learning_rate=0.1, epochs=5, batch_size=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_output(sample_y_pred, sample_y_train, sample_pred_out, act_type):\n",
    "    if act_type == 0: #linear\n",
    "        dO_dNet = 1\n",
    "    elif act_type == 1: #relu\n",
    "        dO_dNet = np.where(sample_y_pred < 0, 0, 1)\n",
    "    elif act_type == 2: #sigmoid\n",
    "        dO_dNet = sample_y_pred * (1 - sample_y_pred)\n",
    "    \n",
    "    # TODO SOFTMAX\n",
    "    \n",
    "    dE_dO = -(sample_y_train - sample_y_pred)\n",
    "    dNet_dW = sample_pred_out                    \n",
    "    dE_dW = dE_dO * dO_dNet * dNet_dW\n",
    "    # print(\"\\n--Calculation---\")\n",
    "    # print(\"dE_dO\")\n",
    "    # print(dE_dO)\n",
    "    # print(\"dO_dNet\")\n",
    "    # print(dO_dNet)\n",
    "    # print(\"dNet_dW\")\n",
    "    # print(dNet_dW)\n",
    "    # print(\"dE_dW\")\n",
    "    # print(dE_dW)\n",
    "\n",
    "    return dE_dW, dE_dO * dO_dNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_hidden(succ_dE_dNet, succ_weight, succ_out, sample_pred_out, act_type):\n",
    "    if act_type == 0: #linear\n",
    "        dO_dNet = 1\n",
    "    elif act_type == 1: #relu\n",
    "        dO_dNet = np.where(sample_y_pred < 0, 0, 1)\n",
    "    elif act_type == 2: #sigmoid\n",
    "        dH_dNet = (succ_out * (1 - succ_out)).flatten()\n",
    "    \n",
    "    # TODO: SOFTMAX\n",
    "\n",
    "    dE_dNet = succ_dE_dNet\n",
    "    dNet_dH = succ_weight\n",
    "    dEtotal_dH = np.sum(dE_dNet * dNet_dH, axis = 1)\n",
    "    dNet_dW = sample_pred_out\n",
    "    dEtotal_dNet = dEtotal_dH * dH_dNet\n",
    "    dEtotal_dW = dEtotal_dNet * dNet_dW\n",
    "\n",
    "    # print(\"dEtotal_dH\")\n",
    "    # print(dEtotal_dH)\n",
    "    # print(\"dH_dNet\")\n",
    "    # print(dH_dNet)\n",
    "    # print(\"dNet_dW\")\n",
    "    # print(dNet_dW)\n",
    "    # print(\"dEtotal_dNet\")\n",
    "    # print(dEtotal_dNet)\n",
    "    # print(\"dEtotal_dW\")\n",
    "    # print(dEtotal_dW)\n",
    "\n",
    "    return dEtotal_dW, dEtotal_dNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([2, 3, 4]), array([23]), array([3, 4, 5])]\n"
     ]
    }
   ],
   "source": [
    "l = [[2,3,4],[23],[3,4,5]]\n",
    "for i in range(len(l)):\n",
    "    l[i] = np.array(l[i])\n",
    "\n",
    "print(l)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
