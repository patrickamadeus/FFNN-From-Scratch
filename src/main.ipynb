{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **NN** Implementation (Forward + Backward Propagation)\n",
    "1. Add input in /test/input and model in /test/model with .txt format, makesure they both have the same filename.\n",
    "2. Input the filename for both the input and the model, ensuring that they have the same filename.\n",
    "3. Scroll down and click on `./tmp/network.html`. This will redirect you to network.html, which you can then open the visualization using a live server.\n",
    "\n",
    "#### Made by:\n",
    "- Samuel Christoper Swandi - 13520075\n",
    "- Grace Claudia - 13520078\n",
    "- Ubaidillah Ariq Prathama - 13520085\n",
    "- Patrick Amadeus Irawan - 13520109\n",
    "\n",
    "------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "List of Content\n",
    "\n",
    "1. [Library & Dependencies](#library)\n",
    "2. [Helper Function](#helper)\n",
    "3. [Neural Network Visualization](#visualization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m pip install pyvis==0.3.2\n",
    "!python3 -m pip install networkx==2.6.3\n",
    "!python3 -m pip install numpy==1.21.6"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Library & Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pandas import DataFrame\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "import os\n",
    "\n",
    "from pyvis.network import Network"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Class**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConnectedLayer:\n",
    "    def __init__(self, input_size, output_size, weights):\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.weights = weights[1:]\n",
    "        self.bias = weights[0]\n",
    "\n",
    "    def forward(self, input):\n",
    "        self.input = input\n",
    "        return np.dot(input, self.weights) + self.bias\n",
    "\n",
    "class ActivationLayer:\n",
    "    def __init__(self, activation):\n",
    "        self.activation = activation\n",
    "    \n",
    "    def forward(self, input):\n",
    "        self.input = input\n",
    "        return self.activation(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_output(sample_y_pred, sample_y_train, sample_pred_out, act_type):\n",
    "    if act_type == \"linear\": #linear\n",
    "        dO_dNet = 1\n",
    "    elif act_type == \"ReLU\": #relu\n",
    "        dO_dNet = np.where(sample_y_pred <= 0, 0, 1)\n",
    "    elif act_type == \"sigmoid\": #sigmoid\n",
    "        dO_dNet = sample_y_pred * (1 - sample_y_pred)\n",
    "\n",
    "    dNet_dW = sample_pred_out                   \n",
    "    if act_type != \"softmax\":\n",
    "        dE_dO = -(sample_y_train - sample_y_pred)\n",
    "        dE_dNet = dE_dO * dO_dNet\n",
    "        dE_dW = dE_dNet * dNet_dW\n",
    "    else:\n",
    "        print(\"I_MAX\", np.argmax(sample_y_train))\n",
    "        print(\"train\",sample_y_train)\n",
    "        print(\"pred\",sample_y_pred)\n",
    "        i_max = np.argmax(sample_y_train)\n",
    "\n",
    "        dE_dNet = []\n",
    "        for y_i in range(len(sample_y_train)):\n",
    "            if y_i == i_max:\n",
    "                print(\"de_dnet di imax\", -(1-sample_y_pred[y_i]))\n",
    "                dE_dNet.append(-(1-sample_y_pred[y_i]))\n",
    "            else:\n",
    "                print(\"de_dnet di ilain\", sample_y_pred[y_i])\n",
    "                dE_dNet.append(sample_y_pred[y_i])\n",
    "        \n",
    "        dE_dNet = np.array(dE_dNet)\n",
    "        dE_dW = dE_dNet * dNet_dW\n",
    "\n",
    "    print(f\"\\n--Calculation {act_type}---\")\n",
    "    # print(\"dE_dO\")\n",
    "    # print(dE_dO)\n",
    "    # print(\"dO_dNet\")\n",
    "    # print(dO_dNet)\n",
    "    print(\"dE_dNet\")\n",
    "    print(dE_dNet)\n",
    "    print(\"dNet_dW\")\n",
    "    print(dNet_dW)\n",
    "    print(\"dE_dW\")\n",
    "    print(dE_dW)\n",
    "\n",
    "    return dE_dW, dE_dNet\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_hidden(succ_dE_dNet, succ_weight, succ_out, sample_pred_out, act_type):\n",
    "    if act_type == \"linear\": #linear\n",
    "        dH_dNet = 1\n",
    "    elif act_type == \"ReLU\": #relu TODO: keknya sala harusnya y, bukan act(y)\n",
    "        dH_dNet = np.where(succ_out <= 0, 0, 1)\n",
    "    elif act_type == \"sigmoid\": #sigmoid\n",
    "        dH_dNet = (succ_out * (1 - succ_out)).flatten()\n",
    "    \n",
    "    # TODO: SOFTMAX\n",
    "\n",
    "    dE_dNet = succ_dE_dNet\n",
    "    dNet_dH = succ_weight\n",
    "    dEtotal_dH = np.sum(dE_dNet * dNet_dH, axis = 1)\n",
    "    dNet_dW = sample_pred_out\n",
    "    dEtotal_dNet = dEtotal_dH * dH_dNet\n",
    "    dEtotal_dW = dEtotal_dNet * dNet_dW\n",
    "\n",
    "    # print(f\"\\n--Calculation {act_type}---\")\n",
    "    # print(\"dE_dNet\")\n",
    "    # print(dE_dNet)\n",
    "    # print(\"dNet_dH\")\n",
    "    # print(dNet_dH)\n",
    "    # print(\"dE_dH\")\n",
    "    # print(dE_dNet * dNet_dH)\n",
    "    # print(\"dEtotal_dH\")\n",
    "    # print(dEtotal_dH)\n",
    "    # print(\"dH_dNet\")\n",
    "    # print(dH_dNet)\n",
    "    # print(\"dNet_dW\")\n",
    "    # print(dNet_dW)\n",
    "    # print(\"dEtotal_dNet\")\n",
    "    # print(dEtotal_dNet)\n",
    "    # print(\"dEtotal_dW\")\n",
    "    # print(dEtotal_dW)\n",
    "\n",
    "    return dEtotal_dW, dEtotal_dNet"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import ceil \n",
    "\n",
    "class NeuralNetwork:\n",
    "    def __init__(self):\n",
    "        self.layers = []\n",
    "\n",
    "    def add(self, layer):\n",
    "        self.layers.append(layer)\n",
    "    \n",
    "    def predict(self, x):\n",
    "        complete_result = []\n",
    "        output = x[:, 1:]\n",
    "\n",
    "        for layer in self.layers:\n",
    "            output = layer.forward(output)\n",
    "            if isinstance(layer, ActivationLayer):\n",
    "                complete_result.append(output)\n",
    "\n",
    "        return complete_result[-1] , complete_result\n",
    "    \n",
    "    def print_weight(self):\n",
    "        i = 0\n",
    "        for layer in self.layers:\n",
    "            if not isinstance(layer, ActivationLayer):\n",
    "                print(\"|____Layer {}___|\".format(i))\n",
    "                print(\"---B I A S---\")\n",
    "                print(layer.bias)\n",
    "                print(\"---N E U R O N S---\")\n",
    "                print(layer.weights)\n",
    "                i+=1\n",
    "            print()\n",
    "\n",
    "    def fit(self, x_train, y_train, learning_rate=0.1, epochs = 5, batch_size = 2, err_threshold = 0.01):\n",
    "        # batch size validation\n",
    "        if batch_size > len(x_train) or batch_size <= 0:\n",
    "            print(\"Batch size {} invalid.\".format(batch_size))\n",
    "            print(\"Batch size range [{}...{}]\".format(1,len(x_train)))\n",
    "            return\n",
    "\n",
    "        print(\"WEIGHT ORI\")\n",
    "        self.print_weight()\n",
    "        print(\"initial error=\", sse(predict_output(x_train, self)[0], y_train))\n",
    "\n",
    "        # print layer type for ActivationLayer only\n",
    "        act_layers = []\n",
    "        for i in range(len(self.layers)):\n",
    "            if isinstance(self.layers[i], ActivationLayer):\n",
    "                act_layers.append(self.layers[i].activation.__name__)\n",
    "        act_layers.reverse()\n",
    "        print(\"\\nActivation Layers From Back to Front: \", act_layers)\n",
    "\n",
    "        if act_layers[0] != \"softmax\":\n",
    "            init_err = sse(predict_output(x_train, self)[0], y_train)\n",
    "        else:\n",
    "            init_err = cross_entropy(np.max(predict_output(x_train, self)[0]))\n",
    "            \n",
    "        # if initial error already below threshold, stop training\n",
    "        if init_err <= err_threshold:\n",
    "            print(\"Initial error already below threshold, no further training needed.\")\n",
    "            return\n",
    "\n",
    "\n",
    "        print()\n",
    "        # training loop\n",
    "        for i in range(epochs):  # Epoch Loop ##################11111111111#############\n",
    "            # print(\"- - - - - - - - - - - - - - - - EPOCH {} - - - - - - - - - - - - - - - -\".format(i+1))\n",
    "            # err = 0\n",
    "            start_i = 0\n",
    "            end_i = batch_size\n",
    "\n",
    "            for j in range(ceil(len(x_train) / batch_size)):  # Mini Batch Loop ###########22222222222############\n",
    "                print(\"============BATCH {}==============\".format(j + 1))\n",
    "                batch_x_train = x_train[start_i:end_i]\n",
    "                batch_y_train = y_train[start_i:end_i]\n",
    "\n",
    "\n",
    "                # FORWARD PROPAGATION\n",
    "                batch_y_pred, complete_out = predict_output(batch_x_train, self)\n",
    "                batch_y_pred = np.array(batch_y_pred)\n",
    "                # err += sse(batch_y_pred, batch_y_train)\n",
    "\n",
    "                # print(\"====FORWARD PROPAGATION====\")\n",
    "                # print(\"Batch X Train\")\n",
    "                # print(batch_x_train)\n",
    "                # print(\"Batch Y Train\")\n",
    "                # print(batch_y_train)\n",
    "                # print(\"Batch Y pred\")\n",
    "                # print(batch_y_pred)\n",
    "\n",
    "                # BACKWARD PROPAGATION\n",
    "                out_grad = None\n",
    "                first_hid_grad = True\n",
    "                hid_grad = []\n",
    "\n",
    "                for k in range(len(batch_x_train)):  # Single Sample Backpropagation Loop ########333333333######\n",
    "                    sample_y_pred = batch_y_pred[k]\n",
    "                    sample_y_train = batch_y_train[k]\n",
    "\n",
    "                    #-------------- Output Gradient --------------#\n",
    "                    if len(self.layers) == 2:\n",
    "                        prev_out = batch_x_train[k][1:].reshape(-1,1)\n",
    "                    else:\n",
    "                        prev_out = uniform2D(np.array(complete_out)[k, -2]).reshape(-1,1)\n",
    "                    \n",
    "                    # Duplicate predecessor to match output layer neuron size & append bias\n",
    "                    succ_size = len(complete_out[0][-1])\n",
    "                    sample_pred_out = np.hstack([prev_out] * succ_size)\n",
    "                    sample_pred_out = np.vstack([np.ones(succ_size),sample_pred_out])\n",
    "\n",
    "                    # print(\"\\n===Batch {} Sample {}===\".format(j+1,k+1))\n",
    "\n",
    "                    # print(\"\\n--Output Materials--\")\n",
    "                    # print(\"sample_y_pred\")\n",
    "                    # print(sample_y_pred)\n",
    "                    # print(\"sample_y_train\")\n",
    "                    # print(sample_y_train)\n",
    "                    # print(\"sample_pred_out\")\n",
    "                    # print(sample_pred_out)\n",
    "                    # print(\"Output Layer Type\")\n",
    "                    # print(act_layers[0])\n",
    "\n",
    "                    dE_dW, succ_dE_dNet = grad_output(sample_y_pred, sample_y_train, sample_pred_out, act_layers[0]) # TODO CHANGE NOT SIGMOID\n",
    "                    if out_grad is None:\n",
    "                        out_grad = dE_dW\n",
    "                    else:\n",
    "                        out_grad += dE_dW\n",
    "\n",
    "                    # -------------- Hidden Gradient --------------#\n",
    "                    hid_grad_i = 0\n",
    "                    reversed_i = -2\n",
    "\n",
    "                    # TODO : Testing\n",
    "                    for l_i in range(len(list_layer_size) - 1):   \n",
    "                        if l_i == len(list_layer_size) - 2:\n",
    "                            prev_out = np.array(batch_x_train[k][1:]).reshape(-1,1)  # cut the bias first\n",
    "                        else:\n",
    "                            prev_out = uniform2D(np.array(complete_out)[k, reversed_i]).reshape(-1,1)\n",
    "                        \n",
    "                        succ_out = np.array(complete_out)[k, reversed_i].reshape(-1,1)\n",
    "                        succ_weight = np.array(self.layers[reversed_i].weights)\n",
    "                        # Duplicate predecessor to match succ layer neuron size & append bias\n",
    "                        succ_size = len(complete_out[0][reversed_i])\n",
    "                        sample_pred_out = np.hstack([prev_out] * succ_size)\n",
    "                        sample_pred_out = np.vstack([np.ones(succ_size),sample_pred_out])\n",
    "\n",
    "                        # print(\"\\n--Hidden Materials--\")\n",
    "                        # print(\"sample_pred_out\")\n",
    "                        # print(sample_pred_out)\n",
    "                        # print(\"succ_out\")\n",
    "                        # print(succ_out)\n",
    "                        # print(\"succ_dE_dNet\")\n",
    "                        # print(succ_dE_dNet)\n",
    "                        # print(\"succ_weight\")\n",
    "                        # print(succ_weight)\n",
    "\n",
    "                        dEtotal_dW, succ_dEtotal_dNet = grad_hidden(succ_dE_dNet, succ_weight, succ_out, sample_pred_out, act_layers[l_i + 1])\n",
    "                        if first_hid_grad:\n",
    "                            hid_grad.append(dEtotal_dW)\n",
    "                        else:\n",
    "                            hid_grad[hid_grad_i] += dEtotal_dW\n",
    "                            hid_grad_i += 1\n",
    "\n",
    "                    first_hid_grad = False\n",
    "                \n",
    "                # print(\"Out grad\")\n",
    "                # print(out_grad)\n",
    "                # print(\"Hid grad\")\n",
    "                # print(hid_grad)\n",
    "\n",
    "                out_delta = -learning_rate * out_grad\n",
    "                hid_delta = -learning_rate * np.array(hid_grad)\n",
    "\n",
    "                # print()\n",
    "                # print(\"Out_delta\")\n",
    "                # print(out_delta)\n",
    "                # print(\"Hid_delta\")\n",
    "                # print(hid_delta)\n",
    "\n",
    "                # reverse hidden delta\n",
    "                deltas = (hid_delta[::-1].tolist() + [out_delta.tolist()])\n",
    "                for d_i in range(len(deltas)):\n",
    "                    deltas[d_i] = np.array(deltas[d_i])\n",
    "                # print(\"Deltas\")\n",
    "                # print(deltas)\n",
    "\n",
    "                # Update weight\n",
    "                d_i = 0\n",
    "                for layer in self.layers:\n",
    "                    if isinstance(layer, ActivationLayer):\n",
    "                        print(layer.activation.__name__)\n",
    "                    if not isinstance(layer, ActivationLayer):\n",
    "                        # print(\"\\n>>>>>>>>>BEFORE\")\n",
    "                        # print(\"Layer Bias\")\n",
    "                        # print(layer.bias)\n",
    "                        # print(\"Layer Weights\")\n",
    "                        # print(layer.weights)\n",
    "                        # print(\"\\nDelta\")\n",
    "                        # print(deltas[d_i][:])\n",
    "                        layer.bias += deltas[d_i][0]\n",
    "                        layer.weights += deltas[d_i][1:]\n",
    "                        # print(\"\\n>>>>>>>>>>AFTER\")\n",
    "                        # print(\"Layer Bias\")\n",
    "                        # print(layer.bias)\n",
    "                        # print(\"Layer Weights\")\n",
    "                        # print(layer.weights)\n",
    "                        d_i+=1  \n",
    "\n",
    "                # Increment next batch\n",
    "                start_i += batch_size\n",
    "                end_i += batch_size\n",
    "                # Verbose Weight per mini-batch\n",
    "                # self.print_weight()\n",
    "            \n",
    "            if act_layers[0] == \"softmax\":\n",
    "                print(\"Output Softmax>>>>>>>>>\")\n",
    "                print(predict_output(x_train, self)[0])\n",
    "                err = cross_entropy(np.max(predict_output(x_train, self)[0]))\n",
    "            else:\n",
    "                err = sse(predict_output(x_train, self)[0], y_train)\n",
    "\n",
    "            print(\"Epoch {}/{}   error={}\".format(i+1,epochs, err))\n",
    "            if err < err_threshold:\n",
    "                print(\"Curr error < threshold. Finishing Epoch\")\n",
    "                break\n",
    "        print(\"\\n----------------------------------------------------------\")\n",
    "        print(\"============|==========FINAL WEIGHT===========|===========\")\n",
    "        print(\"----------------------------------------------------------\")\n",
    "\n",
    "        self.print_weight()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "def uniform2D(arr):\n",
    "    return np.array([np.array(i) for i in arr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear(net):\n",
    "    return net\n",
    "\n",
    "def ReLU(net):\n",
    "    return np.maximum(0,net)\n",
    "\n",
    "def sigmoid(net):\n",
    "    return 1/(1+np.exp(-net))\n",
    "\n",
    "def softmax(net):\n",
    "    res = []\n",
    "    for sample in net:\n",
    "        res.append(np.exp(sample)/np.sum(np.exp(sample)))\n",
    "    return np.array(res)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse(y_true, y_pred):\n",
    "    return np.mean(np.square(y_true - y_pred))\n",
    "\n",
    "def sse(y_true, y_pred):\n",
    "    return np.sum(np.square(y_true - y_pred)) / 2\n",
    "\n",
    "def cross_entropy(pk):\n",
    "    return -np.log(pk)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Loading Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_load(data):\n",
    "    '''\n",
    "    Function to load model from file\n",
    "    INPUT :     data -> data from model file\n",
    "\n",
    "    OUTPUT :    weights -> weights of each model neuron\n",
    "                list_prev_size -> list of previous layer size\n",
    "                list_layer_size -> list of current layer size\n",
    "                list_activation -> list of activation function\n",
    "    '''\n",
    "\n",
    "    idx = 1\n",
    "    weights,list_prev_size,list_layer_size,list_activation = [],[],[],[]\n",
    "\n",
    "    for i in range(int(data[0]) - 1):\n",
    "        # Loading size & act function\n",
    "        prev_size, layer_size, activation = [int(i) for i in data[idx].split()]\n",
    "        list_prev_size.append(prev_size)\n",
    "        list_layer_size.append(layer_size)\n",
    "        list_activation.append(activation)\n",
    "        \n",
    "        # Loading weights\n",
    "        idx += 1\n",
    "        weight = []\n",
    "        for j in range(prev_size + 1):\n",
    "            weight.append([float(i) for i in data[idx].split()])\n",
    "            idx += 1\n",
    "        weights.append(weight)\n",
    "    \n",
    "    # Verbose Information\n",
    "    print('---Model Information---')\n",
    "    print('Number of layers :', len(list_prev_size) + 1)\n",
    "    print('Input size :', list_prev_size[0])\n",
    "    print('Output size :', list_layer_size[-1])\n",
    "\n",
    "    print()\n",
    "    print('Weights :', weights)\n",
    "    print('Previous layer size :', list_prev_size)\n",
    "    print('Current layer size :', list_layer_size)\n",
    "    print('Activation function :', list_activation)\n",
    "\n",
    "    return weights, list_prev_size, list_layer_size, list_activation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Input Loading Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_x_train(input_data):\n",
    "    x_train = []\n",
    "    # Parse each line\n",
    "    for i in range(len(input_data)):\n",
    "        x_train.append([float(i) for i in input_data[i].split()])\n",
    "\n",
    "    return np.array(x_train)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predict Output Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_output(x, net):\n",
    "    '''\n",
    "    Function to predict output from input data\n",
    "    INPUT :     x-> input data\n",
    "                net -> NeuralNetwork object\n",
    "\n",
    "    OUTPUT :    out -> output of the model\n",
    "                complete_out -> complete output of the model, visualization purpose\n",
    "    '''\n",
    "    # Predict output\n",
    "    out, complete_out = net.predict(x)\n",
    "\n",
    "    # Gather complete output\n",
    "    n_complete_out = []\n",
    "    for i in range(len(complete_out[0])):\n",
    "        n_complete_out.append([complete_out[0][i], complete_out[-1][i]])\n",
    "\n",
    "    complete_out = n_complete_out\n",
    "    return out, complete_out"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SSE Errors Function + microhelper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_out_and_errors(output_data, out):\n",
    "    '''\n",
    "    Function to calculate output error\n",
    "    INPUT :     output_data -> output data from output file\n",
    "                out -> output of the model, obtained from predict_output function\n",
    "\n",
    "    OUTPUT :    out_pred -> output of the model (flattened)\n",
    "                out_true -> output from output_data (flattened)\n",
    "                sse_error -> sum squared error\n",
    "                sse_error <= max_sse -> boolean value, True if sse_error <= max_sse, False otherwise\n",
    "    '''\n",
    "    # Assign y_train from output_data\n",
    "    parsed_output = [[float(j) for j in i.split()] for i in output_data]\n",
    "    out_pred = out.flatten()\n",
    "    out_true = np.array(parsed_output[:-1]).flatten()\n",
    "    max_sse = parsed_output[-1][0]\n",
    "\n",
    "    return out_pred, out_true, sse(out_true, out_pred), sse(out_true, out_pred) <= max_sse"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Load** Section"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entrypoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_FOLDER = '../test/model/'\n",
    "INPUT_FOLDER = '../test/input/'\n",
    "OUTPUT_FOLDER = '../test/output/'\n",
    "# read file from test folder\n",
    "def read_file(folder_path, file_name):\n",
    "    with open(folder_path + file_name, 'r') as file:\n",
    "        data = [i.rstrip(\"\\n\") for i in file.readlines()]\n",
    "    return data\n",
    "\n",
    "filename = input('Enter test case name (with extension): ')\n",
    "data = read_file(MODEL_FOLDER, filename)\n",
    "input_data = read_file(INPUT_FOLDER, filename)\n",
    "\n",
    "try: \n",
    "    output_data = read_file(OUTPUT_FOLDER, filename)\n",
    "except:\n",
    "    output_data = None"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model_load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights, list_prev_size, list_layer_size, list_activation = model_load(data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build NN model\n",
    "act = {0: linear, 1: ReLU, 2: sigmoid, 3: softmax}\n",
    "net = NeuralNetwork()\n",
    "for i in range (int(data[0]) - 1):\n",
    "    net.add(ConnectedLayer(list_prev_size[i], list_layer_size[i], weights[i]))\n",
    "    net.add(ActivationLayer(act[list_activation[i]]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Predict** Section"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare x_train & predict output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = generate_x_train(input_data)\n",
    "out, complete_out = predict_output(x_train, net)\n",
    "\n",
    "# Print the information, complete with brief verbose\n",
    "print('---Prediction Information---')\n",
    "print('Input data :', x_train)\n",
    "print('Output data :', out)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute Error with **Sum Squared Error (SSE)**\n",
    "\n",
    "Sum Squared Error (SSE) is a mathematical function used in statistics and machine learning to measure the difference between predicted and actual values. It is commonly used as a cost function in various optimization algorithms, such as gradient descent.\n",
    "\n",
    "The SSE is calculated by taking the difference between each predicted value and its corresponding actual value, squaring the difference, and then summing all of the squared differences:\n",
    "\n",
    "$$SSE = \\sum_{i=1}^{n}(y_i - \\hat{y}_i)^2$$\n",
    "\n",
    "Where:\n",
    "- $n$ is the number of data points\n",
    "- $y_i$ is the actual value of the i-th data point\n",
    "- $\\hat{y}_i$ is the predicted value of the i-th data point\n",
    "\n",
    "The SSE gives an indication of how well the model fits the data. A lower SSE indicates that the model is a better fit for the data.\n",
    "\n",
    "In machine learning, the SSE is often used as a cost function to be minimized during training of a model. The goal is to find the set of model parameters that minimizes the SSE, thus improving the accuracy of the model's predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_pred, out_true, sse_err, isLessThanMaxSSE = compute_out_and_errors(output_data, out)\n",
    "\n",
    "print('Output prediction :', out_pred)\n",
    "print('Output true :', out_true)\n",
    "print('SSE  : ', sse_err)\n",
    "print(\"sse <= max_sse  :\", isLessThanMaxSSE)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network Visualization with **Pyvis**\n",
    "\n",
    "Pyvis is a Python library that provides an easy-to-use interface for visualizing complex networks, including neural networks. Implementation of the visualization is enlisted below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_network(data, complete_out) -> Network:\n",
    "    n = int(data[0])\n",
    "\n",
    "    # Constant\n",
    "    XSTEP, YSTEP, SIZE = 300, 300, 10\n",
    "\n",
    "    # Nodes\n",
    "    nodes = []\n",
    "    node_i = 1\n",
    "\n",
    "    # Nodes Value\n",
    "    value = []\n",
    "    x_val = 0\n",
    "    y_val = 0\n",
    "\n",
    "    # Position\n",
    "    x = []\n",
    "    y = []\n",
    "\n",
    "    # Styling + Text\n",
    "    label = []\n",
    "    color = []\n",
    "    edge = []\n",
    "    title = []\n",
    "\n",
    "    # Indexing\n",
    "    src_idx = 0\n",
    "    idx = 1\n",
    "\n",
    "    for i_layer in range(n - 1):\n",
    "        n_curr, n_next, _ = [int(i) for i in data[idx].split()]\n",
    "\n",
    "        if i_layer == 0:  # means that this is the first layer, hence construct input\n",
    "            for i_node in range(n_curr + 1):\n",
    "                if i_node == 0:  # bias\n",
    "                    color.append(\"#dd4b39\")\n",
    "                    label.append(\"Input[bias]\")\n",
    "                    temp = \"\"\n",
    "                    for i in range (len(input_data)):\n",
    "                        temp += str(x_train[i][0])\n",
    "                        if i < len(input_data) - 1:\n",
    "                            temp += \", \"\n",
    "                    title.append(temp)\n",
    "                else:\n",
    "                    color.append(\"#162347\")\n",
    "                    label.append(\"Input[{}]\".format(i_node))\n",
    "                    temp = \"\"\n",
    "                    for i in range (len(input_data)):\n",
    "                        temp += str(x_train[i][i_node])\n",
    "                        if i < len(input_data) - 1:\n",
    "                            temp += \", \"\n",
    "                    title.append(temp)\n",
    "                value.append(SIZE)\n",
    "                x.append(x_val)\n",
    "                y.append(y_val)\n",
    "                y_val += YSTEP\n",
    "                nodes.append(node_i)\n",
    "                node_i += 1\n",
    "            x_val += XSTEP\n",
    "\n",
    "        y_val = 0\n",
    "        # always construct the next layer\n",
    "        for i_node in range(n_next + 1):\n",
    "            if i_node == 0:\n",
    "                if i_layer == n - 2:\n",
    "                    continue\n",
    "                color.append(\"#dd4b39\")\n",
    "                label.append(\"HL{}[bias]\".format(i_layer + 1))\n",
    "                temp = \"\"\n",
    "                for i in range (len(input_data)):\n",
    "                    temp +=  \"1\"\n",
    "                    if i < len(input_data) - 1:\n",
    "                        temp += \", \"\n",
    "                title.append(temp)\n",
    "            else:\n",
    "                color.append(\"#162347\")\n",
    "                if i_layer == n - 2:\n",
    "                    label.append(\"Output[{}]\".format(i_node))\n",
    "                else:\n",
    "                    label.append(\"HL{}[{}]\".format(i_layer + 1, i_node))\n",
    "                temp = \"\"\n",
    "                for i in range (len(input_data)):\n",
    "                    temp += str(complete_out[i][i_layer][i_node - 1])\n",
    "                    if i < len(input_data) - 1:\n",
    "                        temp += \", \"\n",
    "                title.append(temp)\n",
    "            value.append(SIZE)\n",
    "            x.append(x_val)\n",
    "            y.append(y_val)\n",
    "            y_val += YSTEP\n",
    "            nodes.append(node_i)\n",
    "            node_i += 1\n",
    "        x_val += XSTEP\n",
    "\n",
    "        idx += 1\n",
    "        for origin in range(n_curr + 1):\n",
    "            dst_idx = -1\n",
    "            for w in reversed(data[idx].split()):\n",
    "                edge.append((nodes[src_idx], nodes[dst_idx], w))\n",
    "                dst_idx -= 1\n",
    "            src_idx += 1\n",
    "            idx += 1\n",
    "\n",
    "    g = Network(notebook=True, cdn_resources=\"remote\")\n",
    "    g.add_nodes(nodes,title = title,value = value,x=x,y=y,label = label,color = color)\n",
    "\n",
    "    for e in edge:\n",
    "        g.add_edge(e[0], e[1], title = e[2], color=\"#162347\")\n",
    "\n",
    "    for n in g.nodes:\n",
    "        n.update({'physics': False})\n",
    "    \n",
    "    return g"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize using Helper Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = visualize_network(data, complete_out)\n",
    "g.show(\"./tmp/network.html\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Backpropagation** Section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Model Information---\n",
      "Number of layers : 3\n",
      "Input size : 2\n",
      "Output size : 2\n",
      "\n",
      "Weights : [[[0.1, 0.2], [0.4, -0.5], [0.7, 0.8]], [[0.1, 0.2], [0.4, -0.5], [0.7, 0.8]]]\n",
      "Previous layer size : [2, 2]\n",
      "Current layer size : [2, 2]\n",
      "Activation function : [0, 1]\n",
      "\n",
      "---Prediction Information---\n",
      "Input data : [[ 1.  -1.   0.5]\n",
      " [ 1.   0.5 -1. ]]\n",
      "Output prediction : [0.89  1.055 0.    0.   ]\n",
      "Output true : [0.1 1.  1.  0.1]\n",
      "SSE  :  0.8185625000000001\n",
      "sse <= max_sse  : False\n"
     ]
    }
   ],
   "source": [
    "MODEL_FOLDER = '../test/model/'\n",
    "INPUT_FOLDER = '../test/input/'\n",
    "OUTPUT_FOLDER = '../test/output/'\n",
    "# read file from test folder\n",
    "def read_file(folder_path, file_name):\n",
    "    with open(folder_path + file_name, 'r') as file:\n",
    "        data = [i.rstrip(\"\\n\") for i in file.readlines()]\n",
    "    return data\n",
    "\n",
    "TC_B = \"\"\n",
    "tc_b = input(\"Directly Enter if you wish to access Test Case B folder (otherwise type anything): \")\n",
    "if tc_b == \"\":\n",
    "    MODEL_FOLDER += \"B/\"\n",
    "    INPUT_FOLDER += \"B/\"\n",
    "    OUTPUT_FOLDER += \"B/\"\n",
    "\n",
    "filename = input('Enter test case name (with extension): ')\n",
    "data = read_file(MODEL_FOLDER, filename)\n",
    "input_data = read_file(INPUT_FOLDER, filename)\n",
    "\n",
    "try: \n",
    "    output_data = read_file(OUTPUT_FOLDER, filename)\n",
    "except:\n",
    "    output_data = None\n",
    "\n",
    "# Load model building blocks\n",
    "weights, list_prev_size, list_layer_size, list_activation = model_load(data)\n",
    "\n",
    "# Build NN Model\n",
    "act = {0: linear, 1: ReLU, 2: sigmoid, 3: softmax}\n",
    "net = NeuralNetwork()\n",
    "for i in range (int(data[0]) - 1):\n",
    "    net.add(ConnectedLayer(list_prev_size[i], list_layer_size[i], weights[i]))\n",
    "    net.add(ActivationLayer(act[list_activation[i]]))\n",
    "\n",
    "# Generate x_train\n",
    "x_train = generate_x_train(input_data)\n",
    "out, complete_out = predict_output(x_train, net)\n",
    "\n",
    "# Print the information, complete with brief verbose\n",
    "print('\\n---Prediction Information---')\n",
    "print('Input data :', x_train)\n",
    "\n",
    "# Compute out error\n",
    "out_pred, out_true, sse_err, isLessThanMaxSSE = compute_out_and_errors(output_data, out)\n",
    "\n",
    "print('Output prediction :', out_pred)\n",
    "print('Output true :', out_true)\n",
    "print('SSE  : ', sse_err)\n",
    "print(\"sse <= max_sse  :\", isLessThanMaxSSE)\n",
    "\n",
    "\n",
    "x_train = x_train\n",
    "y_train = out_true.reshape(len(x_train), list_layer_size[-1])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run if process input from iris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load iris import datasets first\n",
    "from sklearn import datasets\n",
    "iris = datasets.load_iris()\n",
    "\n",
    "x_train = np.insert(iris.data, 0, 1, axis=1)[:5]\n",
    "y_train = iris.target.reshape(-1,1)[:5]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run if process input from .txt (tc folder), not iris dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train [[ 1.  -1.   0.5]\n",
      " [ 1.   0.5 -1. ]]\n",
      "y_train [[0.1 1. ]\n",
      " [1.  0.1]]\n"
     ]
    }
   ],
   "source": [
    "x_train = x_train\n",
    "y_train = out_true.reshape(len(x_train), list_layer_size[-1])\n",
    "print(\"x_train\",x_train)\n",
    "print(\"y_train\",y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WEIGHT ORI\n",
      "|____Layer 0___|\n",
      "---B I A S---\n",
      "[0.1, 0.2]\n",
      "---N E U R O N S---\n",
      "[[0.4, -0.5], [0.7, 0.8]]\n",
      "\n",
      "\n",
      "|____Layer 1___|\n",
      "---B I A S---\n",
      "[0.1, 0.2]\n",
      "---N E U R O N S---\n",
      "[[0.4, -0.5], [0.7, 0.8]]\n",
      "\n",
      "\n",
      "initial error= 0.8185625000000001\n",
      "\n",
      "Activation Layers From Back to Front:  ['ReLU', 'linear']\n",
      "\n",
      "============BATCH 1==============\n",
      "\n",
      "--Calculation ReLU---\n",
      "dE_dNet\n",
      "[0.79  0.055]\n",
      "dNet_dW\n",
      "[[1.   1.  ]\n",
      " [0.05 0.05]\n",
      " [1.1  1.1 ]]\n",
      "dE_dW\n",
      "[[0.79    0.055  ]\n",
      " [0.0395  0.00275]\n",
      " [0.869   0.0605 ]]\n",
      "\n",
      "--Calculation ReLU---\n",
      "dE_dNet\n",
      "[-0. -0.]\n",
      "dNet_dW\n",
      "[[ 1.    1.  ]\n",
      " [-0.4  -0.4 ]\n",
      " [-0.85 -0.85]]\n",
      "dE_dW\n",
      "[[-0. -0.]\n",
      " [ 0.  0.]\n",
      " [ 0.  0.]]\n",
      "linear\n",
      "ReLU\n",
      "Epoch 1/1   error=0.6340913392462494\n",
      "\n",
      "----------------------------------------------------------\n",
      "============|==========FINAL WEIGHT===========|===========\n",
      "----------------------------------------------------------\n",
      "|____Layer 0___|\n",
      "---B I A S---\n",
      "[0.07115 0.1403 ]\n",
      "---N E U R O N S---\n",
      "[[ 0.42885  -0.4403  ]\n",
      " [ 0.685575  0.77015 ]]\n",
      "\n",
      "\n",
      "|____Layer 1___|\n",
      "---B I A S---\n",
      "[0.021  0.1945]\n",
      "---N E U R O N S---\n",
      "[[ 0.39605  -0.500275]\n",
      " [ 0.6131    0.79395 ]]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "act = {0: linear, 1: ReLU, 2: sigmoid, 3: softmax}\n",
    "net = NeuralNetwork()\n",
    "for i in range (int(data[0]) - 1):\n",
    "    net.add(ConnectedLayer(list_prev_size[i], list_layer_size[i], weights[i]))\n",
    "    net.add(ActivationLayer(act[list_activation[i]]))\n",
    "\n",
    "net.fit(x_train, y_train, learning_rate=0.1, epochs=1, batch_size=2, err_threshold=0.0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
